<!DOCTYPE html>
<html devsite="">
  <head>
    <meta name="project_path" value="/java/_project.yaml">
    <meta name="book_path" value="/java/_book.yaml">
  </head>
  <body>
    {% verbatim %}
    <div>
      <article data-uid="com.google.cloud.speech.v1beta1">
<h1 class="page-title">Namespace com.google.cloud.speech.v1beta1
</h1>
  
    <h2 id="classes">Classes
  
  </h2>
      <h3><a class="xref" href="com.google.cloud.speech.v1beta1.AsyncRecognizeMetadata.html">AsyncRecognizeMetadata</a></h3>
      <section><pre><code>Describes the progress of a long-running `AsyncRecognize` call. It is
 included in the `metadata` field of the `Operation` returned by the
 `GetOperation` call of the `google::longrunning::Operations` service.
</code></pre><p>Protobuf type <code>google.cloud.speech.v1beta1.AsyncRecognizeMetadata</code></p>
</section>
      <h3><a class="xref" href="com.google.cloud.speech.v1beta1.AsyncRecognizeMetadata.Builder.html">AsyncRecognizeMetadata.Builder</a></h3>
      <section><pre><code>Describes the progress of a long-running `AsyncRecognize` call. It is
 included in the `metadata` field of the `Operation` returned by the
 `GetOperation` call of the `google::longrunning::Operations` service.
</code></pre><p>Protobuf type <code>google.cloud.speech.v1beta1.AsyncRecognizeMetadata</code></p>
</section>
      <h3><a class="xref" href="com.google.cloud.speech.v1beta1.AsyncRecognizeRequest.html">AsyncRecognizeRequest</a></h3>
      <section><pre><code>The top-level message sent by the client for the `AsyncRecognize` method.
</code></pre><p>Protobuf type <code>google.cloud.speech.v1beta1.AsyncRecognizeRequest</code></p>
</section>
      <h3><a class="xref" href="com.google.cloud.speech.v1beta1.AsyncRecognizeRequest.Builder.html">AsyncRecognizeRequest.Builder</a></h3>
      <section><pre><code>The top-level message sent by the client for the `AsyncRecognize` method.
</code></pre><p>Protobuf type <code>google.cloud.speech.v1beta1.AsyncRecognizeRequest</code></p>
</section>
      <h3><a class="xref" href="com.google.cloud.speech.v1beta1.AsyncRecognizeResponse.html">AsyncRecognizeResponse</a></h3>
      <section><pre><code>The only message returned to the client by `AsyncRecognize`. It contains the
 result as zero or more sequential `SpeechRecognitionResult` messages. It is
 included in the `result.response` field of the `Operation` returned by the
 `GetOperation` call of the `google::longrunning::Operations` service.
</code></pre><p>Protobuf type <code>google.cloud.speech.v1beta1.AsyncRecognizeResponse</code></p>
</section>
      <h3><a class="xref" href="com.google.cloud.speech.v1beta1.AsyncRecognizeResponse.Builder.html">AsyncRecognizeResponse.Builder</a></h3>
      <section><pre><code>The only message returned to the client by `AsyncRecognize`. It contains the
 result as zero or more sequential `SpeechRecognitionResult` messages. It is
 included in the `result.response` field of the `Operation` returned by the
 `GetOperation` call of the `google::longrunning::Operations` service.
</code></pre><p>Protobuf type <code>google.cloud.speech.v1beta1.AsyncRecognizeResponse</code></p>
</section>
      <h3><a class="xref" href="com.google.cloud.speech.v1beta1.RecognitionAudio.html">RecognitionAudio</a></h3>
      <section><pre><code>Contains audio data in the encoding specified in the `RecognitionConfig`.
 Either `content` or `uri` must be supplied. Supplying both or neither
 returns [google.rpc.Code.INVALID_ARGUMENT][google.rpc.Code.INVALID_ARGUMENT]. See
 [audio limits](https://cloud.google.com/speech/limits#content).
</code></pre><p>Protobuf type <code>google.cloud.speech.v1beta1.RecognitionAudio</code></p>
</section>
      <h3><a class="xref" href="com.google.cloud.speech.v1beta1.RecognitionAudio.Builder.html">RecognitionAudio.Builder</a></h3>
      <section><pre><code>Contains audio data in the encoding specified in the `RecognitionConfig`.
 Either `content` or `uri` must be supplied. Supplying both or neither
 returns [google.rpc.Code.INVALID_ARGUMENT][google.rpc.Code.INVALID_ARGUMENT]. See
 [audio limits](https://cloud.google.com/speech/limits#content).
</code></pre><p>Protobuf type <code>google.cloud.speech.v1beta1.RecognitionAudio</code></p>
</section>
      <h3><a class="xref" href="com.google.cloud.speech.v1beta1.RecognitionConfig.html">RecognitionConfig</a></h3>
      <section><pre><code>Provides information to the recognizer that specifies how to process the
 request.
</code></pre><p>Protobuf type <code>google.cloud.speech.v1beta1.RecognitionConfig</code></p>
</section>
      <h3><a class="xref" href="com.google.cloud.speech.v1beta1.RecognitionConfig.Builder.html">RecognitionConfig.Builder</a></h3>
      <section><pre><code>Provides information to the recognizer that specifies how to process the
 request.
</code></pre><p>Protobuf type <code>google.cloud.speech.v1beta1.RecognitionConfig</code></p>
</section>
      <h3><a class="xref" href="com.google.cloud.speech.v1beta1.SpeechContext.html">SpeechContext</a></h3>
      <section><pre><code>Provides &quot;hints&quot; to the speech recognizer to favor specific words and phrases
 in the results.
</code></pre><p>Protobuf type <code>google.cloud.speech.v1beta1.SpeechContext</code></p>
</section>
      <h3><a class="xref" href="com.google.cloud.speech.v1beta1.SpeechContext.Builder.html">SpeechContext.Builder</a></h3>
      <section><pre><code>Provides &quot;hints&quot; to the speech recognizer to favor specific words and phrases
 in the results.
</code></pre><p>Protobuf type <code>google.cloud.speech.v1beta1.SpeechContext</code></p>
</section>
      <h3><a class="xref" href="com.google.cloud.speech.v1beta1.SpeechGrpc.html">SpeechGrpc</a></h3>
      <section><pre><code>Service that implements Google Cloud Speech API.
</code></pre></section>
      <h3><a class="xref" href="com.google.cloud.speech.v1beta1.SpeechGrpc.SpeechBlockingStub.html">SpeechGrpc.SpeechBlockingStub</a></h3>
      <section><pre><code>Service that implements Google Cloud Speech API.
</code></pre></section>
      <h3><a class="xref" href="com.google.cloud.speech.v1beta1.SpeechGrpc.SpeechFutureStub.html">SpeechGrpc.SpeechFutureStub</a></h3>
      <section><pre><code>Service that implements Google Cloud Speech API.
</code></pre></section>
      <h3><a class="xref" href="com.google.cloud.speech.v1beta1.SpeechGrpc.SpeechImplBase.html">SpeechGrpc.SpeechImplBase</a></h3>
      <section><pre><code>Service that implements Google Cloud Speech API.
</code></pre></section>
      <h3><a class="xref" href="com.google.cloud.speech.v1beta1.SpeechGrpc.SpeechStub.html">SpeechGrpc.SpeechStub</a></h3>
      <section><pre><code>Service that implements Google Cloud Speech API.
</code></pre></section>
      <h3><a class="xref" href="com.google.cloud.speech.v1beta1.SpeechProto.html">SpeechProto</a></h3>
      <section></section>
      <h3><a class="xref" href="com.google.cloud.speech.v1beta1.SpeechRecognitionAlternative.html">SpeechRecognitionAlternative</a></h3>
      <section><pre><code>Alternative hypotheses (a.k.a. n-best list).
</code></pre><p>Protobuf type <code>google.cloud.speech.v1beta1.SpeechRecognitionAlternative</code></p>
</section>
      <h3><a class="xref" href="com.google.cloud.speech.v1beta1.SpeechRecognitionAlternative.Builder.html">SpeechRecognitionAlternative.Builder</a></h3>
      <section><pre><code>Alternative hypotheses (a.k.a. n-best list).
</code></pre><p>Protobuf type <code>google.cloud.speech.v1beta1.SpeechRecognitionAlternative</code></p>
</section>
      <h3><a class="xref" href="com.google.cloud.speech.v1beta1.SpeechRecognitionResult.html">SpeechRecognitionResult</a></h3>
      <section><pre><code>A speech recognition result corresponding to a portion of the audio.
</code></pre><p>Protobuf type <code>google.cloud.speech.v1beta1.SpeechRecognitionResult</code></p>
</section>
      <h3><a class="xref" href="com.google.cloud.speech.v1beta1.SpeechRecognitionResult.Builder.html">SpeechRecognitionResult.Builder</a></h3>
      <section><pre><code>A speech recognition result corresponding to a portion of the audio.
</code></pre><p>Protobuf type <code>google.cloud.speech.v1beta1.SpeechRecognitionResult</code></p>
</section>
      <h3><a class="xref" href="com.google.cloud.speech.v1beta1.StreamingRecognitionConfig.html">StreamingRecognitionConfig</a></h3>
      <section><pre><code>Provides information to the recognizer that specifies how to process the
 request.
</code></pre><p>Protobuf type <code>google.cloud.speech.v1beta1.StreamingRecognitionConfig</code></p>
</section>
      <h3><a class="xref" href="com.google.cloud.speech.v1beta1.StreamingRecognitionConfig.Builder.html">StreamingRecognitionConfig.Builder</a></h3>
      <section><pre><code>Provides information to the recognizer that specifies how to process the
 request.
</code></pre><p>Protobuf type <code>google.cloud.speech.v1beta1.StreamingRecognitionConfig</code></p>
</section>
      <h3><a class="xref" href="com.google.cloud.speech.v1beta1.StreamingRecognitionResult.html">StreamingRecognitionResult</a></h3>
      <section><pre><code>A streaming speech recognition result corresponding to a portion of the audio
 that is currently being processed.
</code></pre><p>Protobuf type <code>google.cloud.speech.v1beta1.StreamingRecognitionResult</code></p>
</section>
      <h3><a class="xref" href="com.google.cloud.speech.v1beta1.StreamingRecognitionResult.Builder.html">StreamingRecognitionResult.Builder</a></h3>
      <section><pre><code>A streaming speech recognition result corresponding to a portion of the audio
 that is currently being processed.
</code></pre><p>Protobuf type <code>google.cloud.speech.v1beta1.StreamingRecognitionResult</code></p>
</section>
      <h3><a class="xref" href="com.google.cloud.speech.v1beta1.StreamingRecognizeRequest.html">StreamingRecognizeRequest</a></h3>
      <section><pre><code>The top-level message sent by the client for the `StreamingRecognize` method.
 Multiple `StreamingRecognizeRequest` messages are sent. The first message
 must contain a `streaming_config` message and must not contain `audio` data.
 All subsequent messages must contain `audio` data and must not contain a
 `streaming_config` message.
</code></pre><p>Protobuf type <code>google.cloud.speech.v1beta1.StreamingRecognizeRequest</code></p>
</section>
      <h3><a class="xref" href="com.google.cloud.speech.v1beta1.StreamingRecognizeRequest.Builder.html">StreamingRecognizeRequest.Builder</a></h3>
      <section><pre><code>The top-level message sent by the client for the `StreamingRecognize` method.
 Multiple `StreamingRecognizeRequest` messages are sent. The first message
 must contain a `streaming_config` message and must not contain `audio` data.
 All subsequent messages must contain `audio` data and must not contain a
 `streaming_config` message.
</code></pre><p>Protobuf type <code>google.cloud.speech.v1beta1.StreamingRecognizeRequest</code></p>
</section>
      <h3><a class="xref" href="com.google.cloud.speech.v1beta1.StreamingRecognizeResponse.html">StreamingRecognizeResponse</a></h3>
      <section><pre><code>`StreamingRecognizeResponse` is the only message returned to the client by
 `StreamingRecognize`. A series of one or more `StreamingRecognizeResponse`
 messages are streamed back to the client.
 Here&#39;s an example of a series of ten `StreamingRecognizeResponse`s that might
 be returned while processing audio:
 1. endpointer_type: START_OF_SPEECH
 2. results { alternatives { transcript: &quot;tube&quot; } stability: 0.01 }
    result_index: 0
 3. results { alternatives { transcript: &quot;to be a&quot; } stability: 0.01 }
    result_index: 0
 4. results { alternatives { transcript: &quot;to be&quot; } stability: 0.9 }
    results { alternatives { transcript: &quot; or not to be&quot; } stability: 0.01 }
    result_index: 0
 5. results { alternatives { transcript: &quot;to be or not to be&quot;
                             confidence: 0.92 }
              alternatives { transcript: &quot;to bee or not to bee&quot; }
              is_final: true }
    result_index: 0
 6. results { alternatives { transcript: &quot; that&#39;s&quot; } stability: 0.01 }
    result_index: 1
 7. results { alternatives { transcript: &quot; that is&quot; } stability: 0.9 }
    results { alternatives { transcript: &quot; the question&quot; } stability: 0.01 }
    result_index: 1
 8. endpointer_type: END_OF_SPEECH
 9. results { alternatives { transcript: &quot; that is the question&quot;
                             confidence: 0.98 }
              alternatives { transcript: &quot; that was the question&quot; }
              is_final: true }
    result_index: 1
 10. endpointer_type: END_OF_AUDIO
 Notes:
 - Only two of the above responses #5 and #9 contain final results, they are
   indicated by `is_final: true`. Concatenating these together generates the
   full transcript: &quot;to be or not to be that is the question&quot;.
 - The others contain interim `results`. #4 and #7 contain two interim
   `results`, the first portion has a high stability and is less likely to
   change, the second portion has a low stability and is very likely to
   change. A UI designer might choose to show only high stability `results`.
 - The specific `stability` and `confidence` values shown above are only for
   illustrative purposes. Actual values may vary.
 - The `result_index` indicates the portion of audio that has had final
   results returned, and is no longer being processed. For example, the
   `results` in #6 and later correspond to the portion of audio after
   &quot;to be or not to be&quot;.
</code></pre><p>Protobuf type <code>google.cloud.speech.v1beta1.StreamingRecognizeResponse</code></p>
</section>
      <h3><a class="xref" href="com.google.cloud.speech.v1beta1.StreamingRecognizeResponse.Builder.html">StreamingRecognizeResponse.Builder</a></h3>
      <section><pre><code>`StreamingRecognizeResponse` is the only message returned to the client by
 `StreamingRecognize`. A series of one or more `StreamingRecognizeResponse`
 messages are streamed back to the client.
 Here&#39;s an example of a series of ten `StreamingRecognizeResponse`s that might
 be returned while processing audio:
 1. endpointer_type: START_OF_SPEECH
 2. results { alternatives { transcript: &quot;tube&quot; } stability: 0.01 }
    result_index: 0
 3. results { alternatives { transcript: &quot;to be a&quot; } stability: 0.01 }
    result_index: 0
 4. results { alternatives { transcript: &quot;to be&quot; } stability: 0.9 }
    results { alternatives { transcript: &quot; or not to be&quot; } stability: 0.01 }
    result_index: 0
 5. results { alternatives { transcript: &quot;to be or not to be&quot;
                             confidence: 0.92 }
              alternatives { transcript: &quot;to bee or not to bee&quot; }
              is_final: true }
    result_index: 0
 6. results { alternatives { transcript: &quot; that&#39;s&quot; } stability: 0.01 }
    result_index: 1
 7. results { alternatives { transcript: &quot; that is&quot; } stability: 0.9 }
    results { alternatives { transcript: &quot; the question&quot; } stability: 0.01 }
    result_index: 1
 8. endpointer_type: END_OF_SPEECH
 9. results { alternatives { transcript: &quot; that is the question&quot;
                             confidence: 0.98 }
              alternatives { transcript: &quot; that was the question&quot; }
              is_final: true }
    result_index: 1
 10. endpointer_type: END_OF_AUDIO
 Notes:
 - Only two of the above responses #5 and #9 contain final results, they are
   indicated by `is_final: true`. Concatenating these together generates the
   full transcript: &quot;to be or not to be that is the question&quot;.
 - The others contain interim `results`. #4 and #7 contain two interim
   `results`, the first portion has a high stability and is less likely to
   change, the second portion has a low stability and is very likely to
   change. A UI designer might choose to show only high stability `results`.
 - The specific `stability` and `confidence` values shown above are only for
   illustrative purposes. Actual values may vary.
 - The `result_index` indicates the portion of audio that has had final
   results returned, and is no longer being processed. For example, the
   `results` in #6 and later correspond to the portion of audio after
   &quot;to be or not to be&quot;.
</code></pre><p>Protobuf type <code>google.cloud.speech.v1beta1.StreamingRecognizeResponse</code></p>
</section>
      <h3><a class="xref" href="com.google.cloud.speech.v1beta1.SyncRecognizeRequest.html">SyncRecognizeRequest</a></h3>
      <section><pre><code>The top-level message sent by the client for the `SyncRecognize` method.
</code></pre><p>Protobuf type <code>google.cloud.speech.v1beta1.SyncRecognizeRequest</code></p>
</section>
      <h3><a class="xref" href="com.google.cloud.speech.v1beta1.SyncRecognizeRequest.Builder.html">SyncRecognizeRequest.Builder</a></h3>
      <section><pre><code>The top-level message sent by the client for the `SyncRecognize` method.
</code></pre><p>Protobuf type <code>google.cloud.speech.v1beta1.SyncRecognizeRequest</code></p>
</section>
      <h3><a class="xref" href="com.google.cloud.speech.v1beta1.SyncRecognizeResponse.html">SyncRecognizeResponse</a></h3>
      <section><pre><code>The only message returned to the client by `SyncRecognize`. method. It
 contains the result as zero or more sequential `SpeechRecognitionResult`
 messages.
</code></pre><p>Protobuf type <code>google.cloud.speech.v1beta1.SyncRecognizeResponse</code></p>
</section>
      <h3><a class="xref" href="com.google.cloud.speech.v1beta1.SyncRecognizeResponse.Builder.html">SyncRecognizeResponse.Builder</a></h3>
      <section><pre><code>The only message returned to the client by `SyncRecognize`. method. It
 contains the result as zero or more sequential `SpeechRecognitionResult`
 messages.
</code></pre><p>Protobuf type <code>google.cloud.speech.v1beta1.SyncRecognizeResponse</code></p>
</section>
    <h2 id="interfaces">Interfaces
  
  </h2>
      <h3><a class="xref" href="com.google.cloud.speech.v1beta1.AsyncRecognizeMetadataOrBuilder.html">AsyncRecognizeMetadataOrBuilder</a></h3>
      <section></section>
      <h3><a class="xref" href="com.google.cloud.speech.v1beta1.AsyncRecognizeRequestOrBuilder.html">AsyncRecognizeRequestOrBuilder</a></h3>
      <section></section>
      <h3><a class="xref" href="com.google.cloud.speech.v1beta1.AsyncRecognizeResponseOrBuilder.html">AsyncRecognizeResponseOrBuilder</a></h3>
      <section></section>
      <h3><a class="xref" href="com.google.cloud.speech.v1beta1.RecognitionAudioOrBuilder.html">RecognitionAudioOrBuilder</a></h3>
      <section></section>
      <h3><a class="xref" href="com.google.cloud.speech.v1beta1.RecognitionConfigOrBuilder.html">RecognitionConfigOrBuilder</a></h3>
      <section></section>
      <h3><a class="xref" href="com.google.cloud.speech.v1beta1.SpeechContextOrBuilder.html">SpeechContextOrBuilder</a></h3>
      <section></section>
      <h3><a class="xref" href="com.google.cloud.speech.v1beta1.SpeechRecognitionAlternativeOrBuilder.html">SpeechRecognitionAlternativeOrBuilder</a></h3>
      <section></section>
      <h3><a class="xref" href="com.google.cloud.speech.v1beta1.SpeechRecognitionResultOrBuilder.html">SpeechRecognitionResultOrBuilder</a></h3>
      <section></section>
      <h3><a class="xref" href="com.google.cloud.speech.v1beta1.StreamingRecognitionConfigOrBuilder.html">StreamingRecognitionConfigOrBuilder</a></h3>
      <section></section>
      <h3><a class="xref" href="com.google.cloud.speech.v1beta1.StreamingRecognitionResultOrBuilder.html">StreamingRecognitionResultOrBuilder</a></h3>
      <section></section>
      <h3><a class="xref" href="com.google.cloud.speech.v1beta1.StreamingRecognizeRequestOrBuilder.html">StreamingRecognizeRequestOrBuilder</a></h3>
      <section></section>
      <h3><a class="xref" href="com.google.cloud.speech.v1beta1.StreamingRecognizeResponseOrBuilder.html">StreamingRecognizeResponseOrBuilder</a></h3>
      <section></section>
      <h3><a class="xref" href="com.google.cloud.speech.v1beta1.SyncRecognizeRequestOrBuilder.html">SyncRecognizeRequestOrBuilder</a></h3>
      <section></section>
      <h3><a class="xref" href="com.google.cloud.speech.v1beta1.SyncRecognizeResponseOrBuilder.html">SyncRecognizeResponseOrBuilder</a></h3>
      <section></section>
    <h2 id="enums">Enums
  
  </h2>
      <h3><a class="xref" href="com.google.cloud.speech.v1beta1.RecognitionAudio.AudioSourceCase.html">RecognitionAudio.AudioSourceCase</a></h3>
      <section></section>
      <h3><a class="xref" href="com.google.cloud.speech.v1beta1.RecognitionConfig.AudioEncoding.html">RecognitionConfig.AudioEncoding</a></h3>
      <section><pre><code>Audio encoding of the data sent in the audio message. All encodings support
 only 1 channel (mono) audio. Only `FLAC` includes a header that describes
 the bytes of audio that follow the header. The other encodings are raw
 audio bytes with no header.
 For best results, the audio source should be captured and transmitted using
 a lossless encoding (`FLAC` or `LINEAR16`). Recognition accuracy may be
 reduced if lossy codecs (such as AMR, AMR_WB and MULAW) are used to capture
 or transmit the audio, particularly if background noise is present.
</code></pre><p>Protobuf enum <code>google.cloud.speech.v1beta1.RecognitionConfig.AudioEncoding</code></p>
</section>
      <h3><a class="xref" href="com.google.cloud.speech.v1beta1.StreamingRecognizeRequest.StreamingRequestCase.html">StreamingRecognizeRequest.StreamingRequestCase</a></h3>
      <section></section>
      <h3><a class="xref" href="com.google.cloud.speech.v1beta1.StreamingRecognizeResponse.EndpointerType.html">StreamingRecognizeResponse.EndpointerType</a></h3>
      <section><pre><code>Indicates the type of endpointer event.
</code></pre><p>Protobuf enum <code>google.cloud.speech.v1beta1.StreamingRecognizeResponse.EndpointerType</code></p>
</section>
</article>
    </div>
    {% endverbatim %}
  </body>
</html>
