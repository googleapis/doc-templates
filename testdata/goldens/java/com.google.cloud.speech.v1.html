<!DOCTYPE html>
<html devsite="">
  <head>
    <meta name="project_path" value="/java/docs/reference/_project.yaml">
    <meta name="book_path" value="/java/docs/reference/cloud.google.com/java/latest/_book.yaml">
  </head>
  <body>
    {% verbatim %}
    <div>
      <article data-uid="com.google.cloud.speech.v1">
<h1 class="page-title">Package com.google.cloud.speech.v1 (2.0.0)</h1>
  
  <div class="markdown level0 summary"><p>The interfaces provided are listed below, along with usage samples.</p>
<p> <p><h2> SpeechClient </h2></p>
<p> <p>Service Description: Service that implements Google Cloud Speech API.</p>
<p> <p>Sample for SpeechClient:</p>
 <pre class="prettyprint lang-java"><code>
 try (SpeechClient speechClient = SpeechClient.create()) {
   RecognitionConfig config = RecognitionConfig.newBuilder().build();
   RecognitionAudio audio = RecognitionAudio.newBuilder().build();
   RecognizeResponse response = speechClient.recognize(config, audio);
 }
 </code></pre></div>
    <h2 id="classes">Classes
  
  </h2>
      <h3><a class="xref" href="com.google.cloud.speech.v1.LongRunningRecognizeMetadata.html">LongRunningRecognizeMetadata</a></h3>
      <section><p> Describes the progress of a long-running <code>LongRunningRecognize</code> call. It is
 included in the <code>metadata</code> field of the <code>Operation</code> returned by the
 <code>GetOperation</code> call of the <code>google::longrunning::Operations</code> service.</p>
<p> Protobuf type <code>google.cloud.speech.v1.LongRunningRecognizeMetadata</code></p>
</section>
      <h3><a class="xref" href="com.google.cloud.speech.v1.LongRunningRecognizeMetadata.Builder.html">LongRunningRecognizeMetadata.Builder</a></h3>
      <section><p> Describes the progress of a long-running <code>LongRunningRecognize</code> call. It is
 included in the <code>metadata</code> field of the <code>Operation</code> returned by the
 <code>GetOperation</code> call of the <code>google::longrunning::Operations</code> service.</p>
<p> Protobuf type <code>google.cloud.speech.v1.LongRunningRecognizeMetadata</code></p>
</section>
      <h3><a class="xref" href="com.google.cloud.speech.v1.LongRunningRecognizeRequest.html">LongRunningRecognizeRequest</a></h3>
      <section><p> The top-level message sent by the client for the <code>LongRunningRecognize</code>
 method.</p>
<p> Protobuf type <code>google.cloud.speech.v1.LongRunningRecognizeRequest</code></p>
</section>
      <h3><a class="xref" href="com.google.cloud.speech.v1.LongRunningRecognizeRequest.Builder.html">LongRunningRecognizeRequest.Builder</a></h3>
      <section><p> The top-level message sent by the client for the <code>LongRunningRecognize</code>
 method.</p>
<p> Protobuf type <code>google.cloud.speech.v1.LongRunningRecognizeRequest</code></p>
</section>
      <h3><a class="xref" href="com.google.cloud.speech.v1.LongRunningRecognizeResponse.html">LongRunningRecognizeResponse</a></h3>
      <section><p> The only message returned to the client by the <code>LongRunningRecognize</code> method.
 It contains the result as zero or more sequential <code>SpeechRecognitionResult</code>
 messages. It is included in the <code>result.response</code> field of the <code>Operation</code>
 returned by the <code>GetOperation</code> call of the <code>google::longrunning::Operations</code>
 service.</p>
<p> Protobuf type <code>google.cloud.speech.v1.LongRunningRecognizeResponse</code></p>
</section>
      <h3><a class="xref" href="com.google.cloud.speech.v1.LongRunningRecognizeResponse.Builder.html">LongRunningRecognizeResponse.Builder</a></h3>
      <section><p> The only message returned to the client by the <code>LongRunningRecognize</code> method.
 It contains the result as zero or more sequential <code>SpeechRecognitionResult</code>
 messages. It is included in the <code>result.response</code> field of the <code>Operation</code>
 returned by the <code>GetOperation</code> call of the <code>google::longrunning::Operations</code>
 service.</p>
<p> Protobuf type <code>google.cloud.speech.v1.LongRunningRecognizeResponse</code></p>
</section>
      <h3><a class="xref" href="com.google.cloud.speech.v1.RecognitionAudio.html">RecognitionAudio</a></h3>
      <section><p> Contains audio data in the encoding specified in the <code>RecognitionConfig</code>.
 Either <code>content</code> or <code>uri</code> must be supplied. Supplying both or neither
 returns <span class="xref">google.rpc.Code.INVALID_ARGUMENT</span>. See
 <a href="https://cloud.google.com/speech-to-text/quotas#content">content limits</a>.</p>
<p> Protobuf type <code>google.cloud.speech.v1.RecognitionAudio</code></p>
</section>
      <h3><a class="xref" href="com.google.cloud.speech.v1.RecognitionAudio.Builder.html">RecognitionAudio.Builder</a></h3>
      <section><p> Contains audio data in the encoding specified in the <code>RecognitionConfig</code>.
 Either <code>content</code> or <code>uri</code> must be supplied. Supplying both or neither
 returns <span class="xref">google.rpc.Code.INVALID_ARGUMENT</span>. See
 <a href="https://cloud.google.com/speech-to-text/quotas#content">content limits</a>.</p>
<p> Protobuf type <code>google.cloud.speech.v1.RecognitionAudio</code></p>
</section>
      <h3><a class="xref" href="com.google.cloud.speech.v1.RecognitionConfig.html">RecognitionConfig</a></h3>
      <section><p> Provides information to the recognizer that specifies how to process the
 request.</p>
<p> Protobuf type <code>google.cloud.speech.v1.RecognitionConfig</code></p>
</section>
      <h3><a class="xref" href="com.google.cloud.speech.v1.RecognitionConfig.Builder.html">RecognitionConfig.Builder</a></h3>
      <section><p> Provides information to the recognizer that specifies how to process the
 request.</p>
<p> Protobuf type <code>google.cloud.speech.v1.RecognitionConfig</code></p>
</section>
      <h3><a class="xref" href="com.google.cloud.speech.v1.RecognitionMetadata.html">RecognitionMetadata</a></h3>
      <section><p> Description of audio data to be recognized.</p>
<p> Protobuf type <code>google.cloud.speech.v1.RecognitionMetadata</code></p>
</section>
      <h3><a class="xref" href="com.google.cloud.speech.v1.RecognitionMetadata.Builder.html">RecognitionMetadata.Builder</a></h3>
      <section><p> Description of audio data to be recognized.</p>
<p> Protobuf type <code>google.cloud.speech.v1.RecognitionMetadata</code></p>
</section>
      <h3><a class="xref" href="com.google.cloud.speech.v1.RecognizeRequest.html">RecognizeRequest</a></h3>
      <section><p> The top-level message sent by the client for the <code>Recognize</code> method.</p>
<p> Protobuf type <code>google.cloud.speech.v1.RecognizeRequest</code></p>
</section>
      <h3><a class="xref" href="com.google.cloud.speech.v1.RecognizeRequest.Builder.html">RecognizeRequest.Builder</a></h3>
      <section><p> The top-level message sent by the client for the <code>Recognize</code> method.</p>
<p> Protobuf type <code>google.cloud.speech.v1.RecognizeRequest</code></p>
</section>
      <h3><a class="xref" href="com.google.cloud.speech.v1.RecognizeResponse.html">RecognizeResponse</a></h3>
      <section><p> The only message returned to the client by the <code>Recognize</code> method. It
 contains the result as zero or more sequential <code>SpeechRecognitionResult</code>
 messages.</p>
<p> Protobuf type <code>google.cloud.speech.v1.RecognizeResponse</code></p>
</section>
      <h3><a class="xref" href="com.google.cloud.speech.v1.RecognizeResponse.Builder.html">RecognizeResponse.Builder</a></h3>
      <section><p> The only message returned to the client by the <code>Recognize</code> method. It
 contains the result as zero or more sequential <code>SpeechRecognitionResult</code>
 messages.</p>
<p> Protobuf type <code>google.cloud.speech.v1.RecognizeResponse</code></p>
</section>
      <h3><a class="xref" href="com.google.cloud.speech.v1.SpeakerDiarizationConfig.html">SpeakerDiarizationConfig</a></h3>
      <section><p> Config to enable speaker diarization.</p>
<p> Protobuf type <code>google.cloud.speech.v1.SpeakerDiarizationConfig</code></p>
</section>
      <h3><a class="xref" href="com.google.cloud.speech.v1.SpeakerDiarizationConfig.Builder.html">SpeakerDiarizationConfig.Builder</a></h3>
      <section><p> Config to enable speaker diarization.</p>
<p> Protobuf type <code>google.cloud.speech.v1.SpeakerDiarizationConfig</code></p>
</section>
      <h3><a class="xref" href="com.google.cloud.speech.v1.SpeechClient.html">SpeechClient</a></h3>
      <section><p>Service Description: Service that implements Google Cloud Speech API.</p>
<p> <p>This class provides the ability to make remote calls to the backing service through method
 calls that map to API methods. Sample code to get started:</p>
 <pre class="prettyprint lang-java"><code>
 try (SpeechClient speechClient = SpeechClient.create()) {
   RecognitionConfig config = RecognitionConfig.newBuilder().build();
   RecognitionAudio audio = RecognitionAudio.newBuilder().build();
   RecognizeResponse response = speechClient.recognize(config, audio);
 }
 </code></pre>

<p> <p>Note: close() needs to be called on the SpeechClient object to clean up resources such as
 threads. In the example above, try-with-resources is used, which automatically calls close().</p>
<p> <p>The surface of this class includes several types of Java methods for each of the API&#39;s
 methods:</p>
 <ol>
   <li>A &quot;flattened&quot; method. With this type of method, the fields of the request type have been
       converted into function parameters. It may be the case that not all fields are available as
       parameters, and not every API method will have a flattened method entry point.
   <li>A &quot;request object&quot; method. This type of method only takes one parameter, a request object,
       which must be constructed before the call. Not every API method will have a request object
       method.
   <li>A &quot;callable&quot; method. This type of method takes no parameters and returns an immutable API
       callable object, which can be used to initiate calls to the service.
 </li></li></li></ol>

<p> <p>See the individual methods for example code.</p>
<p> <p>Many parameters require resource names to be formatted in a particular way. To assist with
 these names, this class includes a format method for each type of name, and additionally a parse
 method to extract the individual identifiers contained within names that are returned.</p>
<p> <p>This class can be customized by passing in a custom instance of SpeechSettings to create().
 For example:</p>
<p> <p>To customize credentials:</p>
 <pre class="prettyprint lang-java"><code>
 SpeechSettings speechSettings =
     SpeechSettings.newBuilder()
         .setCredentialsProvider(FixedCredentialsProvider.create(myCredentials))
         .build();
 SpeechClient speechClient = SpeechClient.create(speechSettings);
 </code></pre>

<p> <p>To customize the endpoint:</p>
 <pre class="prettyprint lang-java"><code>
 SpeechSettings speechSettings = SpeechSettings.newBuilder().setEndpoint(myEndpoint).build();
 SpeechClient speechClient = SpeechClient.create(speechSettings);
 </code></pre>

<p> <p>Please refer to the GitHub repository&#39;s samples for more quickstart code snippets.</p>
</section>
      <h3><a class="xref" href="com.google.cloud.speech.v1.SpeechContext.html">SpeechContext</a></h3>
      <section><p> Provides &quot;hints&quot; to the speech recognizer to favor specific words and phrases
 in the results.</p>
<p> Protobuf type <code>google.cloud.speech.v1.SpeechContext</code></p>
</section>
      <h3><a class="xref" href="com.google.cloud.speech.v1.SpeechContext.Builder.html">SpeechContext.Builder</a></h3>
      <section><p> Provides &quot;hints&quot; to the speech recognizer to favor specific words and phrases
 in the results.</p>
<p> Protobuf type <code>google.cloud.speech.v1.SpeechContext</code></p>
</section>
      <h3><a class="xref" href="com.google.cloud.speech.v1.SpeechGrpc.html">SpeechGrpc</a></h3>
      <section><p> Service that implements Google Cloud Speech API.</p>
</section>
      <h3><a class="xref" href="com.google.cloud.speech.v1.SpeechGrpc.SpeechBlockingStub.html">SpeechGrpc.SpeechBlockingStub</a></h3>
      <section><p> Service that implements Google Cloud Speech API.</p>
</section>
      <h3><a class="xref" href="com.google.cloud.speech.v1.SpeechGrpc.SpeechFutureStub.html">SpeechGrpc.SpeechFutureStub</a></h3>
      <section><p> Service that implements Google Cloud Speech API.</p>
</section>
      <h3><a class="xref" href="com.google.cloud.speech.v1.SpeechGrpc.SpeechImplBase.html">SpeechGrpc.SpeechImplBase</a></h3>
      <section><p> Service that implements Google Cloud Speech API.</p>
</section>
      <h3><a class="xref" href="com.google.cloud.speech.v1.SpeechGrpc.SpeechStub.html">SpeechGrpc.SpeechStub</a></h3>
      <section><p> Service that implements Google Cloud Speech API.</p>
</section>
      <h3><a class="xref" href="com.google.cloud.speech.v1.SpeechProto.html">SpeechProto</a></h3>
      <section></section>
      <h3><a class="xref" href="com.google.cloud.speech.v1.SpeechRecognitionAlternative.html">SpeechRecognitionAlternative</a></h3>
      <section><p> Alternative hypotheses (a.k.a. n-best list).</p>
<p> Protobuf type <code>google.cloud.speech.v1.SpeechRecognitionAlternative</code></p>
</section>
      <h3><a class="xref" href="com.google.cloud.speech.v1.SpeechRecognitionAlternative.Builder.html">SpeechRecognitionAlternative.Builder</a></h3>
      <section><p> Alternative hypotheses (a.k.a. n-best list).</p>
<p> Protobuf type <code>google.cloud.speech.v1.SpeechRecognitionAlternative</code></p>
</section>
      <h3><a class="xref" href="com.google.cloud.speech.v1.SpeechRecognitionResult.html">SpeechRecognitionResult</a></h3>
      <section><p> A speech recognition result corresponding to a portion of the audio.</p>
<p> Protobuf type <code>google.cloud.speech.v1.SpeechRecognitionResult</code></p>
</section>
      <h3><a class="xref" href="com.google.cloud.speech.v1.SpeechRecognitionResult.Builder.html">SpeechRecognitionResult.Builder</a></h3>
      <section><p> A speech recognition result corresponding to a portion of the audio.</p>
<p> Protobuf type <code>google.cloud.speech.v1.SpeechRecognitionResult</code></p>
</section>
      <h3><a class="xref" href="com.google.cloud.speech.v1.SpeechSettings.html">SpeechSettings</a></h3>
      <section><p>Settings class to configure an instance of <a class="xref" href="com.google.cloud.speech.v1p1beta1.SpeechClient.html">SpeechClient</a>.</p>
<p> <p>The default instance has everything set to sensible defaults:</p>
 <ul>
   <li>The default service address (speech.googleapis.com) and default port (443) are used.
   <li>Credentials are acquired automatically through Application Default Credentials.
   <li>Retries are configured for idempotent methods but not for non-idempotent methods.
 </li></li></li></ul>

<p> <p>The builder of this class is recursive, so contained classes are themselves builders. When
 build() is called, the tree of builders is called to create the complete settings object.</p>
<p> <p>For example, to set the total timeout of recognize to 30 seconds:</p>
 <pre class="prettyprint lang-java"><code>
 SpeechSettings.Builder speechSettingsBuilder = SpeechSettings.newBuilder();
 speechSettingsBuilder
     .recognizeSettings()
     .setRetrySettings(
         speechSettingsBuilder
             .recognizeSettings()
             .getRetrySettings()
             .toBuilder()
             .setTotalTimeout(Duration.ofSeconds(30))
             .build());
 SpeechSettings speechSettings = speechSettingsBuilder.build();
 </code></pre></section>
      <h3><a class="xref" href="com.google.cloud.speech.v1.SpeechSettings.Builder.html">SpeechSettings.Builder</a></h3>
      <section><p>Builder for SpeechSettings.</p>
</section>
      <h3><a class="xref" href="com.google.cloud.speech.v1.StreamingRecognitionConfig.html">StreamingRecognitionConfig</a></h3>
      <section><p> Provides information to the recognizer that specifies how to process the
 request.</p>
<p> Protobuf type <code>google.cloud.speech.v1.StreamingRecognitionConfig</code></p>
</section>
      <h3><a class="xref" href="com.google.cloud.speech.v1.StreamingRecognitionConfig.Builder.html">StreamingRecognitionConfig.Builder</a></h3>
      <section><p> Provides information to the recognizer that specifies how to process the
 request.</p>
<p> Protobuf type <code>google.cloud.speech.v1.StreamingRecognitionConfig</code></p>
</section>
      <h3><a class="xref" href="com.google.cloud.speech.v1.StreamingRecognitionResult.html">StreamingRecognitionResult</a></h3>
      <section><p> A streaming speech recognition result corresponding to a portion of the audio
 that is currently being processed.</p>
<p> Protobuf type <code>google.cloud.speech.v1.StreamingRecognitionResult</code></p>
</section>
      <h3><a class="xref" href="com.google.cloud.speech.v1.StreamingRecognitionResult.Builder.html">StreamingRecognitionResult.Builder</a></h3>
      <section><p> A streaming speech recognition result corresponding to a portion of the audio
 that is currently being processed.</p>
<p> Protobuf type <code>google.cloud.speech.v1.StreamingRecognitionResult</code></p>
</section>
      <h3><a class="xref" href="com.google.cloud.speech.v1.StreamingRecognizeRequest.html">StreamingRecognizeRequest</a></h3>
      <section><p> The top-level message sent by the client for the <code>StreamingRecognize</code> method.
 Multiple <code>StreamingRecognizeRequest</code> messages are sent. The first message
 must contain a <code>streaming_config</code> message and must not contain
 <code>audio_content</code>. All subsequent messages must contain <code>audio_content</code> and
 must not contain a <code>streaming_config</code> message.</p>
<p> Protobuf type <code>google.cloud.speech.v1.StreamingRecognizeRequest</code></p>
</section>
      <h3><a class="xref" href="com.google.cloud.speech.v1.StreamingRecognizeRequest.Builder.html">StreamingRecognizeRequest.Builder</a></h3>
      <section><p> The top-level message sent by the client for the <code>StreamingRecognize</code> method.
 Multiple <code>StreamingRecognizeRequest</code> messages are sent. The first message
 must contain a <code>streaming_config</code> message and must not contain
 <code>audio_content</code>. All subsequent messages must contain <code>audio_content</code> and
 must not contain a <code>streaming_config</code> message.</p>
<p> Protobuf type <code>google.cloud.speech.v1.StreamingRecognizeRequest</code></p>
</section>
      <h3><a class="xref" href="com.google.cloud.speech.v1.StreamingRecognizeResponse.html">StreamingRecognizeResponse</a></h3>
      <section><p> <code>StreamingRecognizeResponse</code> is the only message returned to the client by
 <code>StreamingRecognize</code>. A series of zero or more <code>StreamingRecognizeResponse</code>
 messages are streamed back to the client. If there is no recognizable
 audio, and <code>single_utterance</code> is set to false, then no messages are streamed
 back to the client.
 Here&#39;s an example of a series of <code>StreamingRecognizeResponse</code>s that might be
 returned while processing audio:</p>
<ol>
<li>results { alternatives { transcript: &quot;tube&quot; } stability: 0.01 }</li>
<li>results { alternatives { transcript: &quot;to be a&quot; } stability: 0.01 }</li>
<li>results { alternatives { transcript: &quot;to be&quot; } stability: 0.9 }
results { alternatives { transcript: &quot; or not to be&quot; } stability: 0.01 }</li>
<li>results { alternatives { transcript: &quot;to be or not to be&quot;
                         confidence: 0.92 }
          alternatives { transcript: &quot;to bee or not to bee&quot; }
          is_final: true }</li>
<li>results { alternatives { transcript: &quot; that&#39;s&quot; } stability: 0.01 }</li>
<li>results { alternatives { transcript: &quot; that is&quot; } stability: 0.9 }
results { alternatives { transcript: &quot; the question&quot; } stability: 0.01 }</li>
<li>results { alternatives { transcript: &quot; that is the question&quot;
                         confidence: 0.98 }
          alternatives { transcript: &quot; that was the question&quot; }
          is_final: true }
Notes:</li>
<li>Only two of the above responses #4 and #7 contain final results; they are
indicated by <code>is_final: true</code>. Concatenating these together generates the
full transcript: &quot;to be or not to be that is the question&quot;.</li>
<li>The others contain interim <code>results</code>. #3 and #6 contain two interim
<code>results</code>: the first portion has a high stability and is less likely to
change; the second portion has a low stability and is very likely to
change. A UI designer might choose to show only high stability <code>results</code>.</li>
<li>The specific <code>stability</code> and <code>confidence</code> values shown above are only for
illustrative purposes. Actual values may vary.</li>
<li>In each response, only one of these fields will be set:
  <code>error</code>,
  <code>speech_event_type</code>, or
  one or more (repeated) <code>results</code>.</li>
</ol>
<p> Protobuf type <code>google.cloud.speech.v1.StreamingRecognizeResponse</code></p>
</section>
      <h3><a class="xref" href="com.google.cloud.speech.v1.StreamingRecognizeResponse.Builder.html">StreamingRecognizeResponse.Builder</a></h3>
      <section><p> <code>StreamingRecognizeResponse</code> is the only message returned to the client by
 <code>StreamingRecognize</code>. A series of zero or more <code>StreamingRecognizeResponse</code>
 messages are streamed back to the client. If there is no recognizable
 audio, and <code>single_utterance</code> is set to false, then no messages are streamed
 back to the client.
 Here&#39;s an example of a series of <code>StreamingRecognizeResponse</code>s that might be
 returned while processing audio:</p>
<ol>
<li>results { alternatives { transcript: &quot;tube&quot; } stability: 0.01 }</li>
<li>results { alternatives { transcript: &quot;to be a&quot; } stability: 0.01 }</li>
<li>results { alternatives { transcript: &quot;to be&quot; } stability: 0.9 }
results { alternatives { transcript: &quot; or not to be&quot; } stability: 0.01 }</li>
<li>results { alternatives { transcript: &quot;to be or not to be&quot;
                         confidence: 0.92 }
          alternatives { transcript: &quot;to bee or not to bee&quot; }
          is_final: true }</li>
<li>results { alternatives { transcript: &quot; that&#39;s&quot; } stability: 0.01 }</li>
<li>results { alternatives { transcript: &quot; that is&quot; } stability: 0.9 }
results { alternatives { transcript: &quot; the question&quot; } stability: 0.01 }</li>
<li>results { alternatives { transcript: &quot; that is the question&quot;
                         confidence: 0.98 }
          alternatives { transcript: &quot; that was the question&quot; }
          is_final: true }
Notes:</li>
<li>Only two of the above responses #4 and #7 contain final results; they are
indicated by <code>is_final: true</code>. Concatenating these together generates the
full transcript: &quot;to be or not to be that is the question&quot;.</li>
<li>The others contain interim <code>results</code>. #3 and #6 contain two interim
<code>results</code>: the first portion has a high stability and is less likely to
change; the second portion has a low stability and is very likely to
change. A UI designer might choose to show only high stability <code>results</code>.</li>
<li>The specific <code>stability</code> and <code>confidence</code> values shown above are only for
illustrative purposes. Actual values may vary.</li>
<li>In each response, only one of these fields will be set:
  <code>error</code>,
  <code>speech_event_type</code>, or
  one or more (repeated) <code>results</code>.</li>
</ol>
<p> Protobuf type <code>google.cloud.speech.v1.StreamingRecognizeResponse</code></p>
</section>
      <h3><a class="xref" href="com.google.cloud.speech.v1.WordInfo.html">WordInfo</a></h3>
      <section><p> Word-specific information for recognized words.</p>
<p> Protobuf type <code>google.cloud.speech.v1.WordInfo</code></p>
</section>
      <h3><a class="xref" href="com.google.cloud.speech.v1.WordInfo.Builder.html">WordInfo.Builder</a></h3>
      <section><p> Word-specific information for recognized words.</p>
<p> Protobuf type <code>google.cloud.speech.v1.WordInfo</code></p>
</section>
    <h2 id="interfaces">Interfaces
  
  </h2>
      <h3><a class="xref" href="com.google.cloud.speech.v1.LongRunningRecognizeMetadataOrBuilder.html">LongRunningRecognizeMetadataOrBuilder</a></h3>
      <section></section>
      <h3><a class="xref" href="com.google.cloud.speech.v1.LongRunningRecognizeRequestOrBuilder.html">LongRunningRecognizeRequestOrBuilder</a></h3>
      <section></section>
      <h3><a class="xref" href="com.google.cloud.speech.v1.LongRunningRecognizeResponseOrBuilder.html">LongRunningRecognizeResponseOrBuilder</a></h3>
      <section></section>
      <h3><a class="xref" href="com.google.cloud.speech.v1.RecognitionAudioOrBuilder.html">RecognitionAudioOrBuilder</a></h3>
      <section></section>
      <h3><a class="xref" href="com.google.cloud.speech.v1.RecognitionConfigOrBuilder.html">RecognitionConfigOrBuilder</a></h3>
      <section></section>
      <h3><a class="xref" href="com.google.cloud.speech.v1.RecognitionMetadataOrBuilder.html">RecognitionMetadataOrBuilder</a></h3>
      <section></section>
      <h3><a class="xref" href="com.google.cloud.speech.v1.RecognizeRequestOrBuilder.html">RecognizeRequestOrBuilder</a></h3>
      <section></section>
      <h3><a class="xref" href="com.google.cloud.speech.v1.RecognizeResponseOrBuilder.html">RecognizeResponseOrBuilder</a></h3>
      <section></section>
      <h3><a class="xref" href="com.google.cloud.speech.v1.SpeakerDiarizationConfigOrBuilder.html">SpeakerDiarizationConfigOrBuilder</a></h3>
      <section></section>
      <h3><a class="xref" href="com.google.cloud.speech.v1.SpeechContextOrBuilder.html">SpeechContextOrBuilder</a></h3>
      <section></section>
      <h3><a class="xref" href="com.google.cloud.speech.v1.SpeechRecognitionAlternativeOrBuilder.html">SpeechRecognitionAlternativeOrBuilder</a></h3>
      <section></section>
      <h3><a class="xref" href="com.google.cloud.speech.v1.SpeechRecognitionResultOrBuilder.html">SpeechRecognitionResultOrBuilder</a></h3>
      <section></section>
      <h3><a class="xref" href="com.google.cloud.speech.v1.StreamingRecognitionConfigOrBuilder.html">StreamingRecognitionConfigOrBuilder</a></h3>
      <section></section>
      <h3><a class="xref" href="com.google.cloud.speech.v1.StreamingRecognitionResultOrBuilder.html">StreamingRecognitionResultOrBuilder</a></h3>
      <section></section>
      <h3><a class="xref" href="com.google.cloud.speech.v1.StreamingRecognizeRequestOrBuilder.html">StreamingRecognizeRequestOrBuilder</a></h3>
      <section></section>
      <h3><a class="xref" href="com.google.cloud.speech.v1.StreamingRecognizeResponseOrBuilder.html">StreamingRecognizeResponseOrBuilder</a></h3>
      <section></section>
      <h3><a class="xref" href="com.google.cloud.speech.v1.WordInfoOrBuilder.html">WordInfoOrBuilder</a></h3>
      <section></section>
    <h2 id="enums">Enums
  
  </h2>
      <h3><a class="xref" href="com.google.cloud.speech.v1.RecognitionAudio.AudioSourceCase.html">RecognitionAudio.AudioSourceCase</a></h3>
      <section></section>
      <h3><a class="xref" href="com.google.cloud.speech.v1.RecognitionConfig.AudioEncoding.html">RecognitionConfig.AudioEncoding</a></h3>
      <section><p> The encoding of the audio data sent in the request.
 All encodings support only 1 channel (mono) audio, unless the
 <code>audio_channel_count</code> and <code>enable_separate_recognition_per_channel</code> fields
 are set.
 For best results, the audio source should be captured and transmitted using
 a lossless encoding (<code>FLAC</code> or <code>LINEAR16</code>). The accuracy of the speech
 recognition can be reduced if lossy codecs are used to capture or transmit
 audio, particularly if background noise is present. Lossy codecs include
 <code>MULAW</code>, <code>AMR</code>, <code>AMR_WB</code>, <code>OGG_OPUS</code>, <code>SPEEX_WITH_HEADER_BYTE</code>, <code>MP3</code>.
 The <code>FLAC</code> and <code>WAV</code> audio file formats include a header that describes the
 included audio content. You can request recognition for <code>WAV</code> files that
 contain either <code>LINEAR16</code> or <code>MULAW</code> encoded audio.
 If you send <code>FLAC</code> or <code>WAV</code> audio file format in
 your request, you do not need to specify an <code>AudioEncoding</code>; the audio
 encoding format is determined from the file header. If you specify
 an <code>AudioEncoding</code> when you send  send <code>FLAC</code> or <code>WAV</code> audio, the
 encoding configuration must match the encoding described in the audio
 header; otherwise the request returns an
 <span class="xref">google.rpc.Code.INVALID_ARGUMENT</span> error code.</p>
<p> Protobuf enum <code>google.cloud.speech.v1.RecognitionConfig.AudioEncoding</code></p>
</section>
      <h3><a class="xref" href="com.google.cloud.speech.v1.RecognitionMetadata.InteractionType.html">RecognitionMetadata.InteractionType</a></h3>
      <section><p> Use case categories that the audio recognition request can be described
 by.</p>
<p> Protobuf enum <code>google.cloud.speech.v1.RecognitionMetadata.InteractionType</code></p>
</section>
      <h3><a class="xref" href="com.google.cloud.speech.v1.RecognitionMetadata.MicrophoneDistance.html">RecognitionMetadata.MicrophoneDistance</a></h3>
      <section><p> Enumerates the types of capture settings describing an audio file.</p>
<p> Protobuf enum <code>google.cloud.speech.v1.RecognitionMetadata.MicrophoneDistance</code></p>
</section>
      <h3><a class="xref" href="com.google.cloud.speech.v1.RecognitionMetadata.OriginalMediaType.html">RecognitionMetadata.OriginalMediaType</a></h3>
      <section><p> The original media the speech was recorded on.</p>
<p> Protobuf enum <code>google.cloud.speech.v1.RecognitionMetadata.OriginalMediaType</code></p>
</section>
      <h3><a class="xref" href="com.google.cloud.speech.v1.RecognitionMetadata.RecordingDeviceType.html">RecognitionMetadata.RecordingDeviceType</a></h3>
      <section><p> The type of device the speech was recorded with.</p>
<p> Protobuf enum <code>google.cloud.speech.v1.RecognitionMetadata.RecordingDeviceType</code></p>
</section>
      <h3><a class="xref" href="com.google.cloud.speech.v1.StreamingRecognizeRequest.StreamingRequestCase.html">StreamingRecognizeRequest.StreamingRequestCase</a></h3>
      <section></section>
      <h3><a class="xref" href="com.google.cloud.speech.v1.StreamingRecognizeResponse.SpeechEventType.html">StreamingRecognizeResponse.SpeechEventType</a></h3>
      <section><p> Indicates the type of speech event.</p>
<p> Protobuf enum <code>google.cloud.speech.v1.StreamingRecognizeResponse.SpeechEventType</code></p>
</section>
</article>
    </div>
    {% endverbatim %}
  </body>
</html>
