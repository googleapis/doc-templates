### YamlMime:UniversalReference
api_name: []
items:
- attributes: []
  children:
  - google.cloud.bigquery_storage_v1beta2.services.big_query_write.BigQueryWriteClient.__exit__
  - google.cloud.bigquery_storage_v1beta2.services.big_query_write.BigQueryWriteClient.append_rows
  - google.cloud.bigquery_storage_v1beta2.services.big_query_write.BigQueryWriteClient.batch_commit_write_streams
  - google.cloud.bigquery_storage_v1beta2.services.big_query_write.BigQueryWriteClient.common_billing_account_path
  - google.cloud.bigquery_storage_v1beta2.services.big_query_write.BigQueryWriteClient.common_folder_path
  - google.cloud.bigquery_storage_v1beta2.services.big_query_write.BigQueryWriteClient.common_location_path
  - google.cloud.bigquery_storage_v1beta2.services.big_query_write.BigQueryWriteClient.common_organization_path
  - google.cloud.bigquery_storage_v1beta2.services.big_query_write.BigQueryWriteClient.common_project_path
  - google.cloud.bigquery_storage_v1beta2.services.big_query_write.BigQueryWriteClient.create_write_stream
  - google.cloud.bigquery_storage_v1beta2.services.big_query_write.BigQueryWriteClient.finalize_write_stream
  - google.cloud.bigquery_storage_v1beta2.services.big_query_write.BigQueryWriteClient.flush_rows
  - google.cloud.bigquery_storage_v1beta2.services.big_query_write.BigQueryWriteClient.from_service_account_file
  - google.cloud.bigquery_storage_v1beta2.services.big_query_write.BigQueryWriteClient.from_service_account_info
  - google.cloud.bigquery_storage_v1beta2.services.big_query_write.BigQueryWriteClient.from_service_account_json
  - google.cloud.bigquery_storage_v1beta2.services.big_query_write.BigQueryWriteClient.get_mtls_endpoint_and_cert_source
  - google.cloud.bigquery_storage_v1beta2.services.big_query_write.BigQueryWriteClient.get_write_stream
  - google.cloud.bigquery_storage_v1beta2.services.big_query_write.BigQueryWriteClient.parse_common_billing_account_path
  - google.cloud.bigquery_storage_v1beta2.services.big_query_write.BigQueryWriteClient.parse_common_folder_path
  - google.cloud.bigquery_storage_v1beta2.services.big_query_write.BigQueryWriteClient.parse_common_location_path
  - google.cloud.bigquery_storage_v1beta2.services.big_query_write.BigQueryWriteClient.parse_common_organization_path
  - google.cloud.bigquery_storage_v1beta2.services.big_query_write.BigQueryWriteClient.parse_common_project_path
  - google.cloud.bigquery_storage_v1beta2.services.big_query_write.BigQueryWriteClient.parse_table_path
  - google.cloud.bigquery_storage_v1beta2.services.big_query_write.BigQueryWriteClient.parse_write_stream_path
  - google.cloud.bigquery_storage_v1beta2.services.big_query_write.BigQueryWriteClient.table_path
  - google.cloud.bigquery_storage_v1beta2.services.big_query_write.BigQueryWriteClient.transport
  - google.cloud.bigquery_storage_v1beta2.services.big_query_write.BigQueryWriteClient.write_stream_path
  class: google.cloud.bigquery_storage_v1beta2.services.big_query_write.BigQueryWriteClient
  fullName: google.cloud.bigquery_storage_v1beta2.services.big_query_write.BigQueryWriteClient
  inheritance:
  - type: builtins.object
  langs:
  - python
  module: google.cloud.bigquery_storage_v1beta2.services.big_query_write
  name: BigQueryWriteClient
  source:
    id: BigQueryWriteClient
    path: google/cloud/bigquery_storage_v1beta2/services/big_query_write/client.py
    remote:
      branch: main
      path: google/cloud/bigquery_storage_v1beta2/services/big_query_write/client.py
      repo: git@github.com:googleapis/python-bigquery-storage.git
    startLine: 78
  summary: 'BigQuery Write API.

    The Write API can be used to write data to BigQuery.


    '
  syntax:
    content: 'BigQueryWriteClient(*, credentials: typing.Optional[google.auth.credentials.Credentials]
      = None, transport: typing.Optional[typing.Union[str, google.cloud.bigquery_storage_v1beta2.services.big_query_write.transports.base.BigQueryWriteTransport]]
      = None, client_options: typing.Optional[google.api_core.client_options.ClientOptions]
      = None, client_info: google.api_core.gapic_v1.client_info.ClientInfo = <google.api_core.gapic_v1.client_info.ClientInfo
      object>)'
  type: class
  uid: google.cloud.bigquery_storage_v1beta2.services.big_query_write.BigQueryWriteClient
- attributes: []
  class: google.cloud.bigquery_storage_v1beta2.services.big_query_write.BigQueryWriteClient
  fullName: google.cloud.bigquery_storage_v1beta2.services.big_query_write.BigQueryWriteClient.__exit__
  langs:
  - python
  module: google.cloud.bigquery_storage_v1beta2.services.big_query_write
  name: __exit__
  source:
    id: __exit__
    path: google/cloud/bigquery_storage_v1beta2/services/big_query_write/client.py
    remote:
      branch: main
      path: google/cloud/bigquery_storage_v1beta2/services/big_query_write/client.py
      repo: git@github.com:googleapis/python-bigquery-storage.git
    startLine: 862
  summary: "Releases underlying transport's resources.\n\n.. warning::\n    ONLY use\
    \ as a context manager if the transport is NOT shared\n    with other clients!\
    \ Exiting the with block will CLOSE the transport\n    and may cause errors in\
    \ other clients!\n\n"
  syntax:
    content: __exit__(type, value, traceback)
    parameters: []
  type: method
  uid: google.cloud.bigquery_storage_v1beta2.services.big_query_write.BigQueryWriteClient.__exit__
- attributes: []
  class: google.cloud.bigquery_storage_v1beta2.services.big_query_write.BigQueryWriteClient
  fullName: google.cloud.bigquery_storage_v1beta2.services.big_query_write.BigQueryWriteClient.append_rows
  langs:
  - python
  module: google.cloud.bigquery_storage_v1beta2.services.big_query_write
  name: append_rows
  source:
    id: append_rows
    path: google/cloud/bigquery_storage_v1beta2/services/big_query_write/client.py
    remote:
      branch: main
      path: google/cloud/bigquery_storage_v1beta2/services/big_query_write/client.py
      repo: git@github.com:googleapis/python-bigquery-storage.git
    startLine: 508
  summary: 'Appends data to the given stream.


    If ``offset`` is specified, the ``offset`` is checked against

    the end of stream. The server returns ``OUT_OF_RANGE`` in

    ``AppendRowsResponse`` if an attempt is made to append to an

    offset beyond the current end of the stream or

    ``ALREADY_EXISTS`` if user provids an ``offset`` that has

    already been written to. User can retry with adjusted offset

    within the same RPC stream. If ``offset`` is not specified,

    append happens at the end of the stream.


    The response contains the offset at which the append happened.

    Responses are received in the same order in which requests are

    sent. There will be one response for each successful request. If

    the ``offset`` is not set in response, it means append didn''t

    happen due to some errors. If one request fails, all the

    subsequent requests will also fail until a success request is

    made again.


    If the stream is of ``PENDING`` type, data will only be

    available for read operations after the stream is committed.

    '
  syntax:
    content: "append_rows(\n    requests: typing.Optional[\n        typing.Iterator[\n\
      \            google.cloud.bigquery_storage_v1beta2.types.storage.AppendRowsRequest\n\
      \        ]\n    ] = None,\n    *,\n    retry: typing.Union[\n        google.api_core.retry.Retry,\
      \ google.api_core.gapic_v1.method._MethodDefault\n    ] = _MethodDefault._DEFAULT_VALUE,\n\
      \    timeout: typing.Optional[float] = None,\n    metadata: typing.Sequence[typing.Tuple[str,\
      \ str]] = ()\n)"
    parameters:
    - defaultValue: None
      description: The request object iterator. Request message for `AppendRows`.
      id: requests
      var_type: Iterator[<xref uid="google.cloud.bigquery_storage_v1beta2.types.AppendRowsRequest">google.cloud.bigquery_storage_v1beta2.types.AppendRowsRequest</xref>]
    - description: Designation of what errors, if any, should be retried.
      id: retry
      var_type: google.api_core.retry.Retry
    - description: The timeout for this request.
      id: timeout
      var_type: float
    - description: Strings which should be sent along with the request as metadata.
      id: metadata
      var_type: Sequence[Tuple[str, str]]
    returns:
    - description: Response message for AppendRows.
      var_type: Iterable[<xref uid="google.cloud.bigquery_storage_v1beta2.types.AppendRowsResponse">google.cloud.bigquery_storage_v1beta2.types.AppendRowsResponse</xref>]
  type: method
  uid: google.cloud.bigquery_storage_v1beta2.services.big_query_write.BigQueryWriteClient.append_rows
- attributes: []
  class: google.cloud.bigquery_storage_v1beta2.services.big_query_write.BigQueryWriteClient
  fullName: google.cloud.bigquery_storage_v1beta2.services.big_query_write.BigQueryWriteClient.batch_commit_write_streams
  langs:
  - python
  module: google.cloud.bigquery_storage_v1beta2.services.big_query_write
  name: batch_commit_write_streams
  source:
    id: batch_commit_write_streams
    path: google/cloud/bigquery_storage_v1beta2/services/big_query_write/client.py
    remote:
      branch: main
      path: google/cloud/bigquery_storage_v1beta2/services/big_query_write/client.py
      repo: git@github.com:googleapis/python-bigquery-storage.git
    startLine: 708
  summary: 'Atomically commits a group of ``PENDING`` streams that belong to

    the same ``parent`` table. Streams must be finalized before

    commit and cannot be committed multiple times. Once a stream is

    committed, data in the stream becomes available for read

    operations.

    '
  syntax:
    content: "batch_commit_write_streams(\n    request: typing.Optional[\n       \
      \ typing.Union[\n            google.cloud.bigquery_storage_v1beta2.types.storage.BatchCommitWriteStreamsRequest,\n\
      \            dict,\n        ]\n    ] = None,\n    *,\n    parent: typing.Optional[str]\
      \ = None,\n    retry: typing.Union[\n        google.api_core.retry.Retry, google.api_core.gapic_v1.method._MethodDefault\n\
      \    ] = _MethodDefault._DEFAULT_VALUE,\n    timeout: typing.Optional[float]\
      \ = None,\n    metadata: typing.Sequence[typing.Tuple[str, str]] = ()\n)"
    parameters:
    - defaultValue: None
      description: The request object. Request message for `BatchCommitWriteStreams`.
      id: request
      var_type: Union[<xref uid="google.cloud.bigquery_storage_v1beta2.types.BatchCommitWriteStreamsRequest">google.cloud.bigquery_storage_v1beta2.types.BatchCommitWriteStreamsRequest</xref>,
        dict]
    - description: Required. Parent table that all the streams should belong to, in
        the form of ``projects/{project}/datasets/{dataset}/tables/{table}``. This
        corresponds to the ``parent`` field on the ``request`` instance; if ``request``
        is provided, this should not be set.
      id: parent
      var_type: str
    - description: Designation of what errors, if any, should be retried.
      id: retry
      var_type: google.api_core.retry.Retry
    - description: The timeout for this request.
      id: timeout
      var_type: float
    - description: Strings which should be sent along with the request as metadata.
      id: metadata
      var_type: Sequence[Tuple[str, str]]
    returns:
    - description: Response message for BatchCommitWriteStreams.
      var_type: <xref uid="google.cloud.bigquery_storage_v1beta2.types.BatchCommitWriteStreamsResponse">google.cloud.bigquery_storage_v1beta2.types.BatchCommitWriteStreamsResponse</xref>
  type: method
  uid: google.cloud.bigquery_storage_v1beta2.services.big_query_write.BigQueryWriteClient.batch_commit_write_streams
- attributes: []
  class: google.cloud.bigquery_storage_v1beta2.services.big_query_write.BigQueryWriteClient
  fullName: google.cloud.bigquery_storage_v1beta2.services.big_query_write.BigQueryWriteClient.common_billing_account_path
  langs:
  - python
  module: google.cloud.bigquery_storage_v1beta2.services.big_query_write
  name: common_billing_account_path
  source:
    id: common_billing_account_path
    path: google/cloud/bigquery_storage_v1beta2/services/big_query_write/client.py
    remote:
      branch: main
      path: google/cloud/bigquery_storage_v1beta2/services/big_query_write/client.py
      repo: git@github.com:googleapis/python-bigquery-storage.git
    startLine: 197
  summary: 'Returns a fully-qualified billing_account string.


    '
  syntax:
    content: 'common_billing_account_path(billing_account: str)'
    parameters:
    - id: billing_account
      var_type: str
  type: method
  uid: google.cloud.bigquery_storage_v1beta2.services.big_query_write.BigQueryWriteClient.common_billing_account_path
- attributes: []
  class: google.cloud.bigquery_storage_v1beta2.services.big_query_write.BigQueryWriteClient
  fullName: google.cloud.bigquery_storage_v1beta2.services.big_query_write.BigQueryWriteClient.common_folder_path
  langs:
  - python
  module: google.cloud.bigquery_storage_v1beta2.services.big_query_write
  name: common_folder_path
  source:
    id: common_folder_path
    path: google/cloud/bigquery_storage_v1beta2/services/big_query_write/client.py
    remote:
      branch: main
      path: google/cloud/bigquery_storage_v1beta2/services/big_query_write/client.py
      repo: git@github.com:googleapis/python-bigquery-storage.git
    startLine: 210
  summary: 'Returns a fully-qualified folder string.


    '
  syntax:
    content: 'common_folder_path(folder: str)'
    parameters:
    - id: folder
      var_type: str
  type: method
  uid: google.cloud.bigquery_storage_v1beta2.services.big_query_write.BigQueryWriteClient.common_folder_path
- attributes: []
  class: google.cloud.bigquery_storage_v1beta2.services.big_query_write.BigQueryWriteClient
  fullName: google.cloud.bigquery_storage_v1beta2.services.big_query_write.BigQueryWriteClient.common_location_path
  langs:
  - python
  module: google.cloud.bigquery_storage_v1beta2.services.big_query_write
  name: common_location_path
  source:
    id: common_location_path
    path: google/cloud/bigquery_storage_v1beta2/services/big_query_write/client.py
    remote:
      branch: main
      path: google/cloud/bigquery_storage_v1beta2/services/big_query_write/client.py
      repo: git@github.com:googleapis/python-bigquery-storage.git
    startLine: 243
  summary: 'Returns a fully-qualified location string.


    '
  syntax:
    content: 'common_location_path(project: str, location: str)'
    parameters:
    - id: project
      var_type: str
    - id: location
      var_type: str
  type: method
  uid: google.cloud.bigquery_storage_v1beta2.services.big_query_write.BigQueryWriteClient.common_location_path
- attributes: []
  class: google.cloud.bigquery_storage_v1beta2.services.big_query_write.BigQueryWriteClient
  fullName: google.cloud.bigquery_storage_v1beta2.services.big_query_write.BigQueryWriteClient.common_organization_path
  langs:
  - python
  module: google.cloud.bigquery_storage_v1beta2.services.big_query_write
  name: common_organization_path
  source:
    id: common_organization_path
    path: google/cloud/bigquery_storage_v1beta2/services/big_query_write/client.py
    remote:
      branch: main
      path: google/cloud/bigquery_storage_v1beta2/services/big_query_write/client.py
      repo: git@github.com:googleapis/python-bigquery-storage.git
    startLine: 221
  summary: 'Returns a fully-qualified organization string.


    '
  syntax:
    content: 'common_organization_path(organization: str)'
    parameters:
    - id: organization
      var_type: str
  type: method
  uid: google.cloud.bigquery_storage_v1beta2.services.big_query_write.BigQueryWriteClient.common_organization_path
- attributes: []
  class: google.cloud.bigquery_storage_v1beta2.services.big_query_write.BigQueryWriteClient
  fullName: google.cloud.bigquery_storage_v1beta2.services.big_query_write.BigQueryWriteClient.common_project_path
  langs:
  - python
  module: google.cloud.bigquery_storage_v1beta2.services.big_query_write
  name: common_project_path
  source:
    id: common_project_path
    path: google/cloud/bigquery_storage_v1beta2/services/big_query_write/client.py
    remote:
      branch: main
      path: google/cloud/bigquery_storage_v1beta2/services/big_query_write/client.py
      repo: git@github.com:googleapis/python-bigquery-storage.git
    startLine: 232
  summary: 'Returns a fully-qualified project string.


    '
  syntax:
    content: 'common_project_path(project: str)'
    parameters:
    - id: project
      var_type: str
  type: method
  uid: google.cloud.bigquery_storage_v1beta2.services.big_query_write.BigQueryWriteClient.common_project_path
- attributes: []
  class: google.cloud.bigquery_storage_v1beta2.services.big_query_write.BigQueryWriteClient
  fullName: google.cloud.bigquery_storage_v1beta2.services.big_query_write.BigQueryWriteClient.create_write_stream
  langs:
  - python
  module: google.cloud.bigquery_storage_v1beta2.services.big_query_write
  name: create_write_stream
  source:
    id: create_write_stream
    path: google/cloud/bigquery_storage_v1beta2/services/big_query_write/client.py
    remote:
      branch: main
      path: google/cloud/bigquery_storage_v1beta2/services/big_query_write/client.py
      repo: git@github.com:googleapis/python-bigquery-storage.git
    startLine: 421
  summary: 'Creates a write stream to the given table. Additionally, every

    table has a special COMMITTED stream named ''_default'' to which

    data can be written. This stream doesn''t need to be created

    using CreateWriteStream. It is a stream that can be used

    simultaneously by any number of clients. Data written to this

    stream is considered committed as soon as an acknowledgement is

    received.

    '
  syntax:
    content: "create_write_stream(\n    request: typing.Optional[\n        typing.Union[\n\
      \            google.cloud.bigquery_storage_v1beta2.types.storage.CreateWriteStreamRequest,\n\
      \            dict,\n        ]\n    ] = None,\n    *,\n    parent: typing.Optional[str]\
      \ = None,\n    write_stream: typing.Optional[\n        google.cloud.bigquery_storage_v1beta2.types.stream.WriteStream\n\
      \    ] = None,\n    retry: typing.Union[\n        google.api_core.retry.Retry,\
      \ google.api_core.gapic_v1.method._MethodDefault\n    ] = _MethodDefault._DEFAULT_VALUE,\n\
      \    timeout: typing.Optional[float] = None,\n    metadata: typing.Sequence[typing.Tuple[str,\
      \ str]] = ()\n)"
    parameters:
    - defaultValue: None
      description: The request object. Request message for `CreateWriteStream`.
      id: request
      var_type: Union[<xref uid="google.cloud.bigquery_storage_v1beta2.types.CreateWriteStreamRequest">google.cloud.bigquery_storage_v1beta2.types.CreateWriteStreamRequest</xref>,
        dict]
    - description: Required. Reference to the table to which the stream belongs, in
        the format of ``projects/{project}/datasets/{dataset}/tables/{table}``. This
        corresponds to the ``parent`` field on the ``request`` instance; if ``request``
        is provided, this should not be set.
      id: parent
      var_type: str
    - description: Required. Stream to be created. This corresponds to the ``write_stream``
        field on the ``request`` instance; if ``request`` is provided, this should
        not be set.
      id: write_stream
      var_type: <xref uid="google.cloud.bigquery_storage_v1beta2.types.WriteStream">google.cloud.bigquery_storage_v1beta2.types.WriteStream</xref>
    - description: Designation of what errors, if any, should be retried.
      id: retry
      var_type: google.api_core.retry.Retry
    - description: The timeout for this request.
      id: timeout
      var_type: float
    - description: Strings which should be sent along with the request as metadata.
      id: metadata
      var_type: Sequence[Tuple[str, str]]
    returns:
    - description: Information about a single stream that gets data inside the storage
        system.
      var_type: <xref uid="google.cloud.bigquery_storage_v1beta2.types.WriteStream">google.cloud.bigquery_storage_v1beta2.types.WriteStream</xref>
  type: method
  uid: google.cloud.bigquery_storage_v1beta2.services.big_query_write.BigQueryWriteClient.create_write_stream
- attributes: []
  class: google.cloud.bigquery_storage_v1beta2.services.big_query_write.BigQueryWriteClient
  fullName: google.cloud.bigquery_storage_v1beta2.services.big_query_write.BigQueryWriteClient.finalize_write_stream
  langs:
  - python
  module: google.cloud.bigquery_storage_v1beta2.services.big_query_write
  name: finalize_write_stream
  source:
    id: finalize_write_stream
    path: google/cloud/bigquery_storage_v1beta2/services/big_query_write/client.py
    remote:
      branch: main
      path: google/cloud/bigquery_storage_v1beta2/services/big_query_write/client.py
      repo: git@github.com:googleapis/python-bigquery-storage.git
    startLine: 638
  summary: 'Finalize a write stream so that no new data can be appended to

    the stream. Finalize is not supported on the ''_default'' stream.

    '
  syntax:
    content: "finalize_write_stream(\n    request: typing.Optional[\n        typing.Union[\n\
      \            google.cloud.bigquery_storage_v1beta2.types.storage.FinalizeWriteStreamRequest,\n\
      \            dict,\n        ]\n    ] = None,\n    *,\n    name: typing.Optional[str]\
      \ = None,\n    retry: typing.Union[\n        google.api_core.retry.Retry, google.api_core.gapic_v1.method._MethodDefault\n\
      \    ] = _MethodDefault._DEFAULT_VALUE,\n    timeout: typing.Optional[float]\
      \ = None,\n    metadata: typing.Sequence[typing.Tuple[str, str]] = ()\n)"
    parameters:
    - defaultValue: None
      description: The request object. Request message for invoking `FinalizeWriteStream`.
      id: request
      var_type: Union[<xref uid="google.cloud.bigquery_storage_v1beta2.types.FinalizeWriteStreamRequest">google.cloud.bigquery_storage_v1beta2.types.FinalizeWriteStreamRequest</xref>,
        dict]
    - description: Required. Name of the stream to finalize, in the form of ``projects/{project}/datasets/{dataset}/tables/{table}/streams/{stream}``.
        This corresponds to the ``name`` field on the ``request`` instance; if ``request``
        is provided, this should not be set.
      id: name
      var_type: str
    - description: Designation of what errors, if any, should be retried.
      id: retry
      var_type: google.api_core.retry.Retry
    - description: The timeout for this request.
      id: timeout
      var_type: float
    - description: Strings which should be sent along with the request as metadata.
      id: metadata
      var_type: Sequence[Tuple[str, str]]
    returns:
    - description: Response message for FinalizeWriteStream.
      var_type: <xref uid="google.cloud.bigquery_storage_v1beta2.types.FinalizeWriteStreamResponse">google.cloud.bigquery_storage_v1beta2.types.FinalizeWriteStreamResponse</xref>
  type: method
  uid: google.cloud.bigquery_storage_v1beta2.services.big_query_write.BigQueryWriteClient.finalize_write_stream
- attributes: []
  class: google.cloud.bigquery_storage_v1beta2.services.big_query_write.BigQueryWriteClient
  fullName: google.cloud.bigquery_storage_v1beta2.services.big_query_write.BigQueryWriteClient.flush_rows
  langs:
  - python
  module: google.cloud.bigquery_storage_v1beta2.services.big_query_write
  name: flush_rows
  source:
    id: flush_rows
    path: google/cloud/bigquery_storage_v1beta2/services/big_query_write/client.py
    remote:
      branch: main
      path: google/cloud/bigquery_storage_v1beta2/services/big_query_write/client.py
      repo: git@github.com:googleapis/python-bigquery-storage.git
    startLine: 784
  summary: 'Flushes rows to a BUFFERED stream. If users are appending rows

    to BUFFERED stream, flush operation is required in order for the

    rows to become available for reading. A Flush operation flushes

    up to any previously flushed offset in a BUFFERED stream, to the

    offset specified in the request. Flush is not supported on the

    \_default stream, since it is not BUFFERED.

    '
  syntax:
    content: "flush_rows(\n    request: typing.Optional[\n        typing.Union[\n\
      \            google.cloud.bigquery_storage_v1beta2.types.storage.FlushRowsRequest,\
      \ dict\n        ]\n    ] = None,\n    *,\n    write_stream: typing.Optional[str]\
      \ = None,\n    retry: typing.Union[\n        google.api_core.retry.Retry, google.api_core.gapic_v1.method._MethodDefault\n\
      \    ] = _MethodDefault._DEFAULT_VALUE,\n    timeout: typing.Optional[float]\
      \ = None,\n    metadata: typing.Sequence[typing.Tuple[str, str]] = ()\n)"
    parameters:
    - defaultValue: None
      description: The request object. Request message for `FlushRows`.
      id: request
      var_type: Union[<xref uid="google.cloud.bigquery_storage_v1beta2.types.FlushRowsRequest">google.cloud.bigquery_storage_v1beta2.types.FlushRowsRequest</xref>,
        dict]
    - description: Required. The stream that is the target of the flush operation.
        This corresponds to the ``write_stream`` field on the ``request`` instance;
        if ``request`` is provided, this should not be set.
      id: write_stream
      var_type: str
    - description: Designation of what errors, if any, should be retried.
      id: retry
      var_type: google.api_core.retry.Retry
    - description: The timeout for this request.
      id: timeout
      var_type: float
    - description: Strings which should be sent along with the request as metadata.
      id: metadata
      var_type: Sequence[Tuple[str, str]]
    returns:
    - description: Respond message for FlushRows.
      var_type: <xref uid="google.cloud.bigquery_storage_v1beta2.types.FlushRowsResponse">google.cloud.bigquery_storage_v1beta2.types.FlushRowsResponse</xref>
  type: method
  uid: google.cloud.bigquery_storage_v1beta2.services.big_query_write.BigQueryWriteClient.flush_rows
- attributes: []
  class: google.cloud.bigquery_storage_v1beta2.services.big_query_write.BigQueryWriteClient
  fullName: google.cloud.bigquery_storage_v1beta2.services.big_query_write.BigQueryWriteClient.from_service_account_file
  langs:
  - python
  module: google.cloud.bigquery_storage_v1beta2.services.big_query_write
  name: from_service_account_file
  source:
    id: from_service_account_file
    path: google/cloud/bigquery_storage_v1beta2/services/big_query_write/client.py
    remote:
      branch: main
      path: google/cloud/bigquery_storage_v1beta2/services/big_query_write/client.py
      repo: git@github.com:googleapis/python-bigquery-storage.git
    startLine: 135
  summary: "Creates an instance of this client using the provided credentials\n  \
    \  file.\n"
  syntax:
    content: 'from_service_account_file(filename: str, *args, **kwargs)'
    parameters:
    - description: The path to the service account private key json file.
      id: filename
      var_type: str
    - description: Additional arguments to pass to the constructor.
      id: args
      var_type: ''
    - description: Additional arguments to pass to the constructor.
      id: kwargs
      var_type: ''
    returns:
    - description: The constructed client.
      var_type: BigQueryWriteClient
  type: method
  uid: google.cloud.bigquery_storage_v1beta2.services.big_query_write.BigQueryWriteClient.from_service_account_file
- attributes: []
  class: google.cloud.bigquery_storage_v1beta2.services.big_query_write.BigQueryWriteClient
  fullName: google.cloud.bigquery_storage_v1beta2.services.big_query_write.BigQueryWriteClient.from_service_account_info
  langs:
  - python
  module: google.cloud.bigquery_storage_v1beta2.services.big_query_write
  name: from_service_account_info
  source:
    id: from_service_account_info
    path: google/cloud/bigquery_storage_v1beta2/services/big_query_write/client.py
    remote:
      branch: main
      path: google/cloud/bigquery_storage_v1beta2/services/big_query_write/client.py
      repo: git@github.com:googleapis/python-bigquery-storage.git
    startLine: 118
  summary: "Creates an instance of this client using the provided credentials\n  \
    \  info.\n"
  syntax:
    content: 'from_service_account_info(info: dict, *args, **kwargs)'
    parameters:
    - description: The service account private key info.
      id: info
      var_type: dict
    - description: Additional arguments to pass to the constructor.
      id: args
      var_type: ''
    - description: Additional arguments to pass to the constructor.
      id: kwargs
      var_type: ''
    returns:
    - description: The constructed client.
      var_type: BigQueryWriteClient
  type: method
  uid: google.cloud.bigquery_storage_v1beta2.services.big_query_write.BigQueryWriteClient.from_service_account_info
- attributes: []
  class: google.cloud.bigquery_storage_v1beta2.services.big_query_write.BigQueryWriteClient
  fullName: google.cloud.bigquery_storage_v1beta2.services.big_query_write.BigQueryWriteClient.from_service_account_json
  langs:
  - python
  module: google.cloud.bigquery_storage_v1beta2.services.big_query_write
  name: from_service_account_json
  source:
    id: from_service_account_json
    path: google/cloud/bigquery_storage_v1beta2/services/big_query_write/client.py
    remote:
      branch: main
      path: google/cloud/bigquery_storage_v1beta2/services/big_query_write/client.py
      repo: git@github.com:googleapis/python-bigquery-storage.git
    startLine: 135
  summary: "Creates an instance of this client using the provided credentials\n  \
    \  file.\n"
  syntax:
    content: 'from_service_account_json(filename: str, *args, **kwargs)'
    parameters:
    - description: The path to the service account private key json file.
      id: filename
      var_type: str
    - description: Additional arguments to pass to the constructor.
      id: args
      var_type: ''
    - description: Additional arguments to pass to the constructor.
      id: kwargs
      var_type: ''
    returns:
    - description: The constructed client.
      var_type: BigQueryWriteClient
  type: method
  uid: google.cloud.bigquery_storage_v1beta2.services.big_query_write.BigQueryWriteClient.from_service_account_json
- attributes: []
  class: google.cloud.bigquery_storage_v1beta2.services.big_query_write.BigQueryWriteClient
  fullName: google.cloud.bigquery_storage_v1beta2.services.big_query_write.BigQueryWriteClient.get_mtls_endpoint_and_cert_source
  langs:
  - python
  module: google.cloud.bigquery_storage_v1beta2.services.big_query_write
  name: get_mtls_endpoint_and_cert_source
  source:
    id: get_mtls_endpoint_and_cert_source
    path: google/cloud/bigquery_storage_v1beta2/services/big_query_write/client.py
    remote:
      branch: main
      path: google/cloud/bigquery_storage_v1beta2/services/big_query_write/client.py
      repo: git@github.com:googleapis/python-bigquery-storage.git
    startLine: 256
  summary: 'Return the API endpoint and client cert source for mutual TLS.


    The client cert source is determined in the following order:

    (1) if `GOOGLE_API_USE_CLIENT_CERTIFICATE` environment variable is not "true",
    the

    client cert source is None.

    (2) if `client_options.client_cert_source` is provided, use the provided one;
    if the

    default client cert source exists, use the default one; otherwise the client cert

    source is None.


    The API endpoint is determined in the following order:

    (1) if `client_options.api_endpoint` if provided, use the provided one.

    (2) if `GOOGLE_API_USE_CLIENT_CERTIFICATE` environment variable is "always", use
    the

    default mTLS endpoint; if the environment variabel is "never", use the default
    API

    endpoint; otherwise if client cert source exists, use the default mTLS endpoint,
    otherwise

    use the default API endpoint.


    More details can be found at https://google.aip.dev/auth/4114.

    '
  syntax:
    content: "get_mtls_endpoint_and_cert_source(\n    client_options: typing.Optional[\n\
      \        google.api_core.client_options.ClientOptions\n    ] = None,\n)"
    exceptions:
    - description: If any errors happen.
      var_type: google.auth.exceptions.MutualTLSChannelError
    parameters:
    - defaultValue: None
      description: Custom options for the client. Only the `api_endpoint` and `client_cert_source`
        properties may be used in this method.
      id: client_options
      var_type: google.api_core.client_options.ClientOptions
    returns:
    - description: returns the API endpoint and the client cert source to use.
      var_type: Tuple[str, Callable[[], Tuple[bytes, bytes]]]
  type: method
  uid: google.cloud.bigquery_storage_v1beta2.services.big_query_write.BigQueryWriteClient.get_mtls_endpoint_and_cert_source
- attributes: []
  class: google.cloud.bigquery_storage_v1beta2.services.big_query_write.BigQueryWriteClient
  fullName: google.cloud.bigquery_storage_v1beta2.services.big_query_write.BigQueryWriteClient.get_write_stream
  langs:
  - python
  module: google.cloud.bigquery_storage_v1beta2.services.big_query_write
  name: get_write_stream
  source:
    id: get_write_stream
    path: google/cloud/bigquery_storage_v1beta2/services/big_query_write/client.py
    remote:
      branch: main
      path: google/cloud/bigquery_storage_v1beta2/services/big_query_write/client.py
      repo: git@github.com:googleapis/python-bigquery-storage.git
    startLine: 566
  summary: 'Gets a write stream.

    '
  syntax:
    content: "get_write_stream(\n    request: typing.Optional[\n        typing.Union[\n\
      \            google.cloud.bigquery_storage_v1beta2.types.storage.GetWriteStreamRequest,\n\
      \            dict,\n        ]\n    ] = None,\n    *,\n    name: typing.Optional[str]\
      \ = None,\n    retry: typing.Union[\n        google.api_core.retry.Retry, google.api_core.gapic_v1.method._MethodDefault\n\
      \    ] = _MethodDefault._DEFAULT_VALUE,\n    timeout: typing.Optional[float]\
      \ = None,\n    metadata: typing.Sequence[typing.Tuple[str, str]] = ()\n)"
    parameters:
    - defaultValue: None
      description: The request object. Request message for `GetWriteStreamRequest`.
      id: request
      var_type: Union[<xref uid="google.cloud.bigquery_storage_v1beta2.types.GetWriteStreamRequest">google.cloud.bigquery_storage_v1beta2.types.GetWriteStreamRequest</xref>,
        dict]
    - description: Required. Name of the stream to get, in the form of ``projects/{project}/datasets/{dataset}/tables/{table}/streams/{stream}``.
        This corresponds to the ``name`` field on the ``request`` instance; if ``request``
        is provided, this should not be set.
      id: name
      var_type: str
    - description: Designation of what errors, if any, should be retried.
      id: retry
      var_type: google.api_core.retry.Retry
    - description: The timeout for this request.
      id: timeout
      var_type: float
    - description: Strings which should be sent along with the request as metadata.
      id: metadata
      var_type: Sequence[Tuple[str, str]]
    returns:
    - description: Information about a single stream that gets data inside the storage
        system.
      var_type: <xref uid="google.cloud.bigquery_storage_v1beta2.types.WriteStream">google.cloud.bigquery_storage_v1beta2.types.WriteStream</xref>
  type: method
  uid: google.cloud.bigquery_storage_v1beta2.services.big_query_write.BigQueryWriteClient.get_write_stream
- attributes: []
  class: google.cloud.bigquery_storage_v1beta2.services.big_query_write.BigQueryWriteClient
  fullName: google.cloud.bigquery_storage_v1beta2.services.big_query_write.BigQueryWriteClient.parse_common_billing_account_path
  langs:
  - python
  module: google.cloud.bigquery_storage_v1beta2.services.big_query_write
  name: parse_common_billing_account_path
  source:
    id: parse_common_billing_account_path
    path: google/cloud/bigquery_storage_v1beta2/services/big_query_write/client.py
    remote:
      branch: main
      path: google/cloud/bigquery_storage_v1beta2/services/big_query_write/client.py
      repo: git@github.com:googleapis/python-bigquery-storage.git
    startLine: 204
  summary: 'Parse a billing_account path into its component segments.


    '
  syntax:
    content: 'parse_common_billing_account_path(path: str)'
    parameters:
    - id: path
      var_type: str
  type: method
  uid: google.cloud.bigquery_storage_v1beta2.services.big_query_write.BigQueryWriteClient.parse_common_billing_account_path
- attributes: []
  class: google.cloud.bigquery_storage_v1beta2.services.big_query_write.BigQueryWriteClient
  fullName: google.cloud.bigquery_storage_v1beta2.services.big_query_write.BigQueryWriteClient.parse_common_folder_path
  langs:
  - python
  module: google.cloud.bigquery_storage_v1beta2.services.big_query_write
  name: parse_common_folder_path
  source:
    id: parse_common_folder_path
    path: google/cloud/bigquery_storage_v1beta2/services/big_query_write/client.py
    remote:
      branch: main
      path: google/cloud/bigquery_storage_v1beta2/services/big_query_write/client.py
      repo: git@github.com:googleapis/python-bigquery-storage.git
    startLine: 215
  summary: 'Parse a folder path into its component segments.


    '
  syntax:
    content: 'parse_common_folder_path(path: str)'
    parameters:
    - id: path
      var_type: str
  type: method
  uid: google.cloud.bigquery_storage_v1beta2.services.big_query_write.BigQueryWriteClient.parse_common_folder_path
- attributes: []
  class: google.cloud.bigquery_storage_v1beta2.services.big_query_write.BigQueryWriteClient
  fullName: google.cloud.bigquery_storage_v1beta2.services.big_query_write.BigQueryWriteClient.parse_common_location_path
  langs:
  - python
  module: google.cloud.bigquery_storage_v1beta2.services.big_query_write
  name: parse_common_location_path
  source:
    id: parse_common_location_path
    path: google/cloud/bigquery_storage_v1beta2/services/big_query_write/client.py
    remote:
      branch: main
      path: google/cloud/bigquery_storage_v1beta2/services/big_query_write/client.py
      repo: git@github.com:googleapis/python-bigquery-storage.git
    startLine: 250
  summary: 'Parse a location path into its component segments.


    '
  syntax:
    content: 'parse_common_location_path(path: str)'
    parameters:
    - id: path
      var_type: str
  type: method
  uid: google.cloud.bigquery_storage_v1beta2.services.big_query_write.BigQueryWriteClient.parse_common_location_path
- attributes: []
  class: google.cloud.bigquery_storage_v1beta2.services.big_query_write.BigQueryWriteClient
  fullName: google.cloud.bigquery_storage_v1beta2.services.big_query_write.BigQueryWriteClient.parse_common_organization_path
  langs:
  - python
  module: google.cloud.bigquery_storage_v1beta2.services.big_query_write
  name: parse_common_organization_path
  source:
    id: parse_common_organization_path
    path: google/cloud/bigquery_storage_v1beta2/services/big_query_write/client.py
    remote:
      branch: main
      path: google/cloud/bigquery_storage_v1beta2/services/big_query_write/client.py
      repo: git@github.com:googleapis/python-bigquery-storage.git
    startLine: 226
  summary: 'Parse a organization path into its component segments.


    '
  syntax:
    content: 'parse_common_organization_path(path: str)'
    parameters:
    - id: path
      var_type: str
  type: method
  uid: google.cloud.bigquery_storage_v1beta2.services.big_query_write.BigQueryWriteClient.parse_common_organization_path
- attributes: []
  class: google.cloud.bigquery_storage_v1beta2.services.big_query_write.BigQueryWriteClient
  fullName: google.cloud.bigquery_storage_v1beta2.services.big_query_write.BigQueryWriteClient.parse_common_project_path
  langs:
  - python
  module: google.cloud.bigquery_storage_v1beta2.services.big_query_write
  name: parse_common_project_path
  source:
    id: parse_common_project_path
    path: google/cloud/bigquery_storage_v1beta2/services/big_query_write/client.py
    remote:
      branch: main
      path: google/cloud/bigquery_storage_v1beta2/services/big_query_write/client.py
      repo: git@github.com:googleapis/python-bigquery-storage.git
    startLine: 237
  summary: 'Parse a project path into its component segments.


    '
  syntax:
    content: 'parse_common_project_path(path: str)'
    parameters:
    - id: path
      var_type: str
  type: method
  uid: google.cloud.bigquery_storage_v1beta2.services.big_query_write.BigQueryWriteClient.parse_common_project_path
- attributes: []
  class: google.cloud.bigquery_storage_v1beta2.services.big_query_write.BigQueryWriteClient
  fullName: google.cloud.bigquery_storage_v1beta2.services.big_query_write.BigQueryWriteClient.parse_table_path
  langs:
  - python
  module: google.cloud.bigquery_storage_v1beta2.services.big_query_write
  name: parse_table_path
  source:
    id: parse_table_path
    path: google/cloud/bigquery_storage_v1beta2/services/big_query_write/client.py
    remote:
      branch: main
      path: google/cloud/bigquery_storage_v1beta2/services/big_query_write/client.py
      repo: git@github.com:googleapis/python-bigquery-storage.git
    startLine: 172
  summary: 'Parses a table path into its component segments.


    '
  syntax:
    content: 'parse_table_path(path: str)'
    parameters:
    - id: path
      var_type: str
  type: method
  uid: google.cloud.bigquery_storage_v1beta2.services.big_query_write.BigQueryWriteClient.parse_table_path
- attributes: []
  class: google.cloud.bigquery_storage_v1beta2.services.big_query_write.BigQueryWriteClient
  fullName: google.cloud.bigquery_storage_v1beta2.services.big_query_write.BigQueryWriteClient.parse_write_stream_path
  langs:
  - python
  module: google.cloud.bigquery_storage_v1beta2.services.big_query_write
  name: parse_write_stream_path
  source:
    id: parse_write_stream_path
    path: google/cloud/bigquery_storage_v1beta2/services/big_query_write/client.py
    remote:
      branch: main
      path: google/cloud/bigquery_storage_v1beta2/services/big_query_write/client.py
      repo: git@github.com:googleapis/python-bigquery-storage.git
    startLine: 188
  summary: 'Parses a write_stream path into its component segments.


    '
  syntax:
    content: 'parse_write_stream_path(path: str)'
    parameters:
    - id: path
      var_type: str
  type: method
  uid: google.cloud.bigquery_storage_v1beta2.services.big_query_write.BigQueryWriteClient.parse_write_stream_path
- attributes: []
  class: google.cloud.bigquery_storage_v1beta2.services.big_query_write.BigQueryWriteClient
  fullName: google.cloud.bigquery_storage_v1beta2.services.big_query_write.BigQueryWriteClient.table_path
  langs:
  - python
  module: google.cloud.bigquery_storage_v1beta2.services.big_query_write
  name: table_path
  source:
    id: table_path
    path: google/cloud/bigquery_storage_v1beta2/services/big_query_write/client.py
    remote:
      branch: main
      path: google/cloud/bigquery_storage_v1beta2/services/big_query_write/client.py
      repo: git@github.com:googleapis/python-bigquery-storage.git
    startLine: 165
  summary: 'Returns a fully-qualified table string.


    '
  syntax:
    content: 'table_path(project: str, dataset: str, table: str)'
    parameters:
    - id: project
      var_type: str
    - id: dataset
      var_type: str
    - id: table
      var_type: str
  type: method
  uid: google.cloud.bigquery_storage_v1beta2.services.big_query_write.BigQueryWriteClient.table_path
- &id001
  attributes: []
  class: google.cloud.bigquery_storage_v1beta2.services.big_query_write.BigQueryWriteClient
  fullName: google.cloud.bigquery_storage_v1beta2.services.big_query_write.BigQueryWriteClient.transport
  langs:
  - python
  module: google.cloud.bigquery_storage_v1beta2.services.big_query_write
  name: transport
  source:
    id: transport
    path: null
    remote:
      branch: main
      path: null
      repo: git@github.com:googleapis/python-bigquery-storage.git
    startLine: null
  summary: 'Returns the transport used by the client instance.

    '
  syntax:
    returns:
    - description: The transport used by the client instance.
      var_type: BigQueryWriteTransport
  type: property
  uid: google.cloud.bigquery_storage_v1beta2.services.big_query_write.BigQueryWriteClient.transport
- *id001
- attributes: []
  class: google.cloud.bigquery_storage_v1beta2.services.big_query_write.BigQueryWriteClient
  fullName: google.cloud.bigquery_storage_v1beta2.services.big_query_write.BigQueryWriteClient.write_stream_path
  langs:
  - python
  module: google.cloud.bigquery_storage_v1beta2.services.big_query_write
  name: write_stream_path
  source:
    id: write_stream_path
    path: google/cloud/bigquery_storage_v1beta2/services/big_query_write/client.py
    remote:
      branch: main
      path: google/cloud/bigquery_storage_v1beta2/services/big_query_write/client.py
      repo: git@github.com:googleapis/python-bigquery-storage.git
    startLine: 181
  summary: 'Returns a fully-qualified write_stream string.


    '
  syntax:
    content: 'write_stream_path(project: str, dataset: str, table: str, stream: str)'
    parameters:
    - id: project
      var_type: str
    - id: dataset
      var_type: str
    - id: table
      var_type: str
    - id: stream
      var_type: str
  type: method
  uid: google.cloud.bigquery_storage_v1beta2.services.big_query_write.BigQueryWriteClient.write_stream_path
references:
- fullName: google.cloud.bigquery_storage_v1beta2.services.big_query_write.BigQueryWriteClient.__exit__
  isExternal: false
  name: __exit__
  parent: google.cloud.bigquery_storage_v1beta2.services.big_query_write.BigQueryWriteClient
  uid: google.cloud.bigquery_storage_v1beta2.services.big_query_write.BigQueryWriteClient.__exit__
- fullName: google.cloud.bigquery_storage_v1beta2.services.big_query_write.BigQueryWriteClient.append_rows
  isExternal: false
  name: append_rows
  parent: google.cloud.bigquery_storage_v1beta2.services.big_query_write.BigQueryWriteClient
  uid: google.cloud.bigquery_storage_v1beta2.services.big_query_write.BigQueryWriteClient.append_rows
- fullName: google.cloud.bigquery_storage_v1beta2.services.big_query_write.BigQueryWriteClient.batch_commit_write_streams
  isExternal: false
  name: batch_commit_write_streams
  parent: google.cloud.bigquery_storage_v1beta2.services.big_query_write.BigQueryWriteClient
  uid: google.cloud.bigquery_storage_v1beta2.services.big_query_write.BigQueryWriteClient.batch_commit_write_streams
- fullName: google.cloud.bigquery_storage_v1beta2.services.big_query_write.BigQueryWriteClient.common_billing_account_path
  isExternal: false
  name: common_billing_account_path
  parent: google.cloud.bigquery_storage_v1beta2.services.big_query_write.BigQueryWriteClient
  uid: google.cloud.bigquery_storage_v1beta2.services.big_query_write.BigQueryWriteClient.common_billing_account_path
- fullName: google.cloud.bigquery_storage_v1beta2.services.big_query_write.BigQueryWriteClient.common_folder_path
  isExternal: false
  name: common_folder_path
  parent: google.cloud.bigquery_storage_v1beta2.services.big_query_write.BigQueryWriteClient
  uid: google.cloud.bigquery_storage_v1beta2.services.big_query_write.BigQueryWriteClient.common_folder_path
- fullName: google.cloud.bigquery_storage_v1beta2.services.big_query_write.BigQueryWriteClient.common_location_path
  isExternal: false
  name: common_location_path
  parent: google.cloud.bigquery_storage_v1beta2.services.big_query_write.BigQueryWriteClient
  uid: google.cloud.bigquery_storage_v1beta2.services.big_query_write.BigQueryWriteClient.common_location_path
- fullName: google.cloud.bigquery_storage_v1beta2.services.big_query_write.BigQueryWriteClient.common_organization_path
  isExternal: false
  name: common_organization_path
  parent: google.cloud.bigquery_storage_v1beta2.services.big_query_write.BigQueryWriteClient
  uid: google.cloud.bigquery_storage_v1beta2.services.big_query_write.BigQueryWriteClient.common_organization_path
- fullName: google.cloud.bigquery_storage_v1beta2.services.big_query_write.BigQueryWriteClient.common_project_path
  isExternal: false
  name: common_project_path
  parent: google.cloud.bigquery_storage_v1beta2.services.big_query_write.BigQueryWriteClient
  uid: google.cloud.bigquery_storage_v1beta2.services.big_query_write.BigQueryWriteClient.common_project_path
- fullName: google.cloud.bigquery_storage_v1beta2.services.big_query_write.BigQueryWriteClient.create_write_stream
  isExternal: false
  name: create_write_stream
  parent: google.cloud.bigquery_storage_v1beta2.services.big_query_write.BigQueryWriteClient
  uid: google.cloud.bigquery_storage_v1beta2.services.big_query_write.BigQueryWriteClient.create_write_stream
- fullName: google.cloud.bigquery_storage_v1beta2.services.big_query_write.BigQueryWriteClient.finalize_write_stream
  isExternal: false
  name: finalize_write_stream
  parent: google.cloud.bigquery_storage_v1beta2.services.big_query_write.BigQueryWriteClient
  uid: google.cloud.bigquery_storage_v1beta2.services.big_query_write.BigQueryWriteClient.finalize_write_stream
- fullName: google.cloud.bigquery_storage_v1beta2.services.big_query_write.BigQueryWriteClient.flush_rows
  isExternal: false
  name: flush_rows
  parent: google.cloud.bigquery_storage_v1beta2.services.big_query_write.BigQueryWriteClient
  uid: google.cloud.bigquery_storage_v1beta2.services.big_query_write.BigQueryWriteClient.flush_rows
- fullName: google.cloud.bigquery_storage_v1beta2.services.big_query_write.BigQueryWriteClient.from_service_account_file
  isExternal: false
  name: from_service_account_file
  parent: google.cloud.bigquery_storage_v1beta2.services.big_query_write.BigQueryWriteClient
  uid: google.cloud.bigquery_storage_v1beta2.services.big_query_write.BigQueryWriteClient.from_service_account_file
- fullName: google.cloud.bigquery_storage_v1beta2.services.big_query_write.BigQueryWriteClient.from_service_account_info
  isExternal: false
  name: from_service_account_info
  parent: google.cloud.bigquery_storage_v1beta2.services.big_query_write.BigQueryWriteClient
  uid: google.cloud.bigquery_storage_v1beta2.services.big_query_write.BigQueryWriteClient.from_service_account_info
- fullName: google.cloud.bigquery_storage_v1beta2.services.big_query_write.BigQueryWriteClient.from_service_account_json
  isExternal: false
  name: from_service_account_json
  parent: google.cloud.bigquery_storage_v1beta2.services.big_query_write.BigQueryWriteClient
  uid: google.cloud.bigquery_storage_v1beta2.services.big_query_write.BigQueryWriteClient.from_service_account_json
- fullName: google.cloud.bigquery_storage_v1beta2.services.big_query_write.BigQueryWriteClient.get_mtls_endpoint_and_cert_source
  isExternal: false
  name: get_mtls_endpoint_and_cert_source
  parent: google.cloud.bigquery_storage_v1beta2.services.big_query_write.BigQueryWriteClient
  uid: google.cloud.bigquery_storage_v1beta2.services.big_query_write.BigQueryWriteClient.get_mtls_endpoint_and_cert_source
- fullName: google.cloud.bigquery_storage_v1beta2.services.big_query_write.BigQueryWriteClient.get_write_stream
  isExternal: false
  name: get_write_stream
  parent: google.cloud.bigquery_storage_v1beta2.services.big_query_write.BigQueryWriteClient
  uid: google.cloud.bigquery_storage_v1beta2.services.big_query_write.BigQueryWriteClient.get_write_stream
- fullName: google.cloud.bigquery_storage_v1beta2.services.big_query_write.BigQueryWriteClient.parse_common_billing_account_path
  isExternal: false
  name: parse_common_billing_account_path
  parent: google.cloud.bigquery_storage_v1beta2.services.big_query_write.BigQueryWriteClient
  uid: google.cloud.bigquery_storage_v1beta2.services.big_query_write.BigQueryWriteClient.parse_common_billing_account_path
- fullName: google.cloud.bigquery_storage_v1beta2.services.big_query_write.BigQueryWriteClient.parse_common_folder_path
  isExternal: false
  name: parse_common_folder_path
  parent: google.cloud.bigquery_storage_v1beta2.services.big_query_write.BigQueryWriteClient
  uid: google.cloud.bigquery_storage_v1beta2.services.big_query_write.BigQueryWriteClient.parse_common_folder_path
- fullName: google.cloud.bigquery_storage_v1beta2.services.big_query_write.BigQueryWriteClient.parse_common_location_path
  isExternal: false
  name: parse_common_location_path
  parent: google.cloud.bigquery_storage_v1beta2.services.big_query_write.BigQueryWriteClient
  uid: google.cloud.bigquery_storage_v1beta2.services.big_query_write.BigQueryWriteClient.parse_common_location_path
- fullName: google.cloud.bigquery_storage_v1beta2.services.big_query_write.BigQueryWriteClient.parse_common_organization_path
  isExternal: false
  name: parse_common_organization_path
  parent: google.cloud.bigquery_storage_v1beta2.services.big_query_write.BigQueryWriteClient
  uid: google.cloud.bigquery_storage_v1beta2.services.big_query_write.BigQueryWriteClient.parse_common_organization_path
- fullName: google.cloud.bigquery_storage_v1beta2.services.big_query_write.BigQueryWriteClient.parse_common_project_path
  isExternal: false
  name: parse_common_project_path
  parent: google.cloud.bigquery_storage_v1beta2.services.big_query_write.BigQueryWriteClient
  uid: google.cloud.bigquery_storage_v1beta2.services.big_query_write.BigQueryWriteClient.parse_common_project_path
- fullName: google.cloud.bigquery_storage_v1beta2.services.big_query_write.BigQueryWriteClient.parse_table_path
  isExternal: false
  name: parse_table_path
  parent: google.cloud.bigquery_storage_v1beta2.services.big_query_write.BigQueryWriteClient
  uid: google.cloud.bigquery_storage_v1beta2.services.big_query_write.BigQueryWriteClient.parse_table_path
- fullName: google.cloud.bigquery_storage_v1beta2.services.big_query_write.BigQueryWriteClient.parse_write_stream_path
  isExternal: false
  name: parse_write_stream_path
  parent: google.cloud.bigquery_storage_v1beta2.services.big_query_write.BigQueryWriteClient
  uid: google.cloud.bigquery_storage_v1beta2.services.big_query_write.BigQueryWriteClient.parse_write_stream_path
- fullName: google.cloud.bigquery_storage_v1beta2.services.big_query_write.BigQueryWriteClient.table_path
  isExternal: false
  name: table_path
  parent: google.cloud.bigquery_storage_v1beta2.services.big_query_write.BigQueryWriteClient
  uid: google.cloud.bigquery_storage_v1beta2.services.big_query_write.BigQueryWriteClient.table_path
- fullName: google.cloud.bigquery_storage_v1beta2.services.big_query_write.BigQueryWriteClient.transport
  isExternal: false
  name: transport
  parent: google.cloud.bigquery_storage_v1beta2.services.big_query_write.BigQueryWriteClient
  uid: google.cloud.bigquery_storage_v1beta2.services.big_query_write.BigQueryWriteClient.transport
- fullName: google.cloud.bigquery_storage_v1beta2.services.big_query_write.BigQueryWriteClient.write_stream_path
  isExternal: false
  name: write_stream_path
  parent: google.cloud.bigquery_storage_v1beta2.services.big_query_write.BigQueryWriteClient
  uid: google.cloud.bigquery_storage_v1beta2.services.big_query_write.BigQueryWriteClient.write_stream_path
