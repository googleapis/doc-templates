### YamlMime:UniversalReference
api_name: []
items:
- children:
  - google.cloud.bigquery_storage_v1.reader.ReadRowsStream.__iter__
  - google.cloud.bigquery_storage_v1.reader.ReadRowsStream.rows
  - google.cloud.bigquery_storage_v1.reader.ReadRowsStream.to_arrow
  - google.cloud.bigquery_storage_v1.reader.ReadRowsStream.to_dataframe
  class: google.cloud.bigquery_storage_v1.reader.ReadRowsStream
  fullName: google.cloud.bigquery_storage_v1.reader.ReadRowsStream
  inheritance:
  - type: builtins.object
  langs:
  - python
  module: google.cloud.bigquery_storage_v1.reader
  name: ReadRowsStream
  source:
    id: ReadRowsStream
    path: google/cloud/bigquery_storage_v1/reader.py
    remote:
      branch: master
      path: google/cloud/bigquery_storage_v1/reader.py
      repo: git@github.com:googleapis/python-bigquery-storage.git
    startLine: 63
  summary: 'A stream of results from a read rows request.



    This stream is an iterable of

    <xref uid="google.cloud.bigquery_storage_v1.types.ReadRowsResponse">ReadRowsResponse</xref>.

    Iterate over it to fetch all row messages.



    If the fastavro library is installed, use the

    <xref uid="google.cloud.bigquery_storage_v1.reader.ReadRowsStream.rows">rows()</xref>

    method to parse all messages into a stream of row dictionaries.



    If the pandas and fastavro libraries are installed, use the

    <xref uid="google.cloud.bigquery_storage_v1.reader.ReadRowsStream.to_dataframe">to_dataframe()</xref>

    method to parse all messages into a `pandas.DataFrame`.

    '
  syntax:
    content: ReadRowsStream(wrapped, client, name, offset, read_rows_kwargs)
  type: class
  uid: google.cloud.bigquery_storage_v1.reader.ReadRowsStream
- class: google.cloud.bigquery_storage_v1.reader.ReadRowsStream
  fullName: google.cloud.bigquery_storage_v1.reader.ReadRowsStream.__iter__
  langs:
  - python
  module: google.cloud.bigquery_storage_v1.reader
  name: __iter__
  source:
    id: __iter__
    path: google/cloud/bigquery_storage_v1/reader.py
    remote:
      branch: master
      path: google/cloud/bigquery_storage_v1/reader.py
      repo: git@github.com:googleapis/python-bigquery-storage.git
    startLine: 120
  summary: 'An iterable of messages.



    '
  syntax:
    content: __iter__()
    parameters: []
    returns:
    - description: A sequence of row messages.
      var_type: Iterable[ <xref uid="google.cloud.bigquery_storage_v1.types.ReadRowsResponse">ReadRowsResponse</xref>
        ]
  type: method
  uid: google.cloud.bigquery_storage_v1.reader.ReadRowsStream.__iter__
- class: google.cloud.bigquery_storage_v1.reader.ReadRowsStream
  fullName: google.cloud.bigquery_storage_v1.reader.ReadRowsStream.rows
  langs:
  - python
  module: google.cloud.bigquery_storage_v1.reader
  name: rows
  source:
    id: rows
    path: google/cloud/bigquery_storage_v1/reader.py
    remote:
      branch: master
      path: google/cloud/bigquery_storage_v1/reader.py
      repo: git@github.com:googleapis/python-bigquery-storage.git
    startLine: 159
  summary: "Iterate over all rows in the stream.\n\n\nThis method requires the fastavro\
    \ library in order to parse row\nmessages in avro format.  For arrow format messages,\
    \ the pyarrow\nlibrary is required.\n\n\n.. warning::\n    DATETIME columns are\
    \ not supported. They are currently parsed as\n    strings in the fastavro library.\n\
    \n\n"
  syntax:
    content: rows(read_session=None)
    parameters:
    - defaultValue: None
      description: DEPRECATED. This argument was used to specify the schema of the
        rows in the stream, but now the first message in a read stream contains this
        information.
      id: read_session
      var_type: Optional[<xref uid="google.cloud.bigquery_storage_v1.types.ReadSession">ReadSession</xref>]
    returns:
    - description: A sequence of rows, represented as dictionaries.
      var_type: Iterable[Mapping]
  type: method
  uid: google.cloud.bigquery_storage_v1.reader.ReadRowsStream.rows
- class: google.cloud.bigquery_storage_v1.reader.ReadRowsStream
  fullName: google.cloud.bigquery_storage_v1.reader.ReadRowsStream.to_arrow
  langs:
  - python
  module: google.cloud.bigquery_storage_v1.reader
  name: to_arrow
  source:
    id: to_arrow
    path: google/cloud/bigquery_storage_v1/reader.py
    remote:
      branch: master
      path: google/cloud/bigquery_storage_v1/reader.py
      repo: git@github.com:googleapis/python-bigquery-storage.git
    startLine: 186
  summary: 'Create a `pyarrow.Table` of all rows in the stream.



    This method requires the pyarrow library and a stream using the Arrow

    format.



    '
  syntax:
    content: to_arrow(read_session=None)
    parameters:
    - defaultValue: None
      description: DEPRECATED. This argument was used to specify the schema of the
        rows in the stream, but now the first message in a read stream contains this
        information.
      id: read_session
      var_type: <xref uid="google.cloud.bigquery_storage_v1.types.ReadSession">ReadSession</xref>
    returns:
    - description: A table of all rows in the stream.
      var_type: pyarrow.Table
  type: method
  uid: google.cloud.bigquery_storage_v1.reader.ReadRowsStream.to_arrow
- class: google.cloud.bigquery_storage_v1.reader.ReadRowsStream
  fullName: google.cloud.bigquery_storage_v1.reader.ReadRowsStream.to_dataframe
  langs:
  - python
  module: google.cloud.bigquery_storage_v1.reader
  name: to_dataframe
  source:
    id: to_dataframe
    path: google/cloud/bigquery_storage_v1/reader.py
    remote:
      branch: master
      path: google/cloud/bigquery_storage_v1/reader.py
      repo: git@github.com:googleapis/python-bigquery-storage.git
    startLine: 208
  summary: "Create a `pandas.DataFrame` of all rows in the stream.\n\n\nThis method\
    \ requires the pandas libary to create a data frame and the\nfastavro library\
    \ to parse row messages.\n\n\n.. warning::\n    DATETIME columns are not supported.\
    \ They are currently parsed as\n    strings.\n\n\n"
  syntax:
    content: to_dataframe(read_session=None, dtypes=None)
    parameters:
    - defaultValue: None
      description: DEPRECATED. This argument was used to specify the schema of the
        rows in the stream, but now the first message in a read stream contains this
        information.
      id: read_session
      var_type: <xref uid="google.cloud.bigquery_storage_v1.types.ReadSession">ReadSession</xref>
    - defaultValue: None
      description: Optional. A dictionary of column names pandas ``dtype``s. The provided
        ``dtype`` is used when constructing the series for the column specified. Otherwise,
        the default pandas behavior is used.
      id: dtypes
      var_type: Map[str, Union[str, pandas.Series.dtype]]
    returns:
    - description: A data frame of all rows in the stream.
      var_type: pandas.DataFrame
  type: method
  uid: google.cloud.bigquery_storage_v1.reader.ReadRowsStream.to_dataframe
references:
- fullName: google.cloud.bigquery_storage_v1.reader.ReadRowsStream.__iter__
  isExternal: false
  name: __iter__
  parent: google.cloud.bigquery_storage_v1.reader.ReadRowsStream
  uid: google.cloud.bigquery_storage_v1.reader.ReadRowsStream.__iter__
- fullName: google.cloud.bigquery_storage_v1.reader.ReadRowsStream.rows
  isExternal: false
  name: rows
  parent: google.cloud.bigquery_storage_v1.reader.ReadRowsStream
  uid: google.cloud.bigquery_storage_v1.reader.ReadRowsStream.rows
- fullName: google.cloud.bigquery_storage_v1.reader.ReadRowsStream.to_arrow
  isExternal: false
  name: to_arrow
  parent: google.cloud.bigquery_storage_v1.reader.ReadRowsStream
  uid: google.cloud.bigquery_storage_v1.reader.ReadRowsStream.to_arrow
- fullName: google.cloud.bigquery_storage_v1.reader.ReadRowsStream.to_dataframe
  isExternal: false
  name: to_dataframe
  parent: google.cloud.bigquery_storage_v1.reader.ReadRowsStream
  uid: google.cloud.bigquery_storage_v1.reader.ReadRowsStream.to_dataframe
