### YamlMime:UniversalReference
api_name: []
items:
- attributes: []
  children:
  - google.cloud.bigquery_storage_v1.client.BigQueryWriteClient
  - google.cloud.bigquery_storage_v1.client.BigQueryWriteClient.__exit__
  - google.cloud.bigquery_storage_v1.client.BigQueryWriteClient.append_rows
  - google.cloud.bigquery_storage_v1.client.BigQueryWriteClient.batch_commit_write_streams
  - google.cloud.bigquery_storage_v1.client.BigQueryWriteClient.common_billing_account_path
  - google.cloud.bigquery_storage_v1.client.BigQueryWriteClient.common_folder_path
  - google.cloud.bigquery_storage_v1.client.BigQueryWriteClient.common_location_path
  - google.cloud.bigquery_storage_v1.client.BigQueryWriteClient.common_organization_path
  - google.cloud.bigquery_storage_v1.client.BigQueryWriteClient.common_project_path
  - google.cloud.bigquery_storage_v1.client.BigQueryWriteClient.create_write_stream
  - google.cloud.bigquery_storage_v1.client.BigQueryWriteClient.finalize_write_stream
  - google.cloud.bigquery_storage_v1.client.BigQueryWriteClient.flush_rows
  - google.cloud.bigquery_storage_v1.client.BigQueryWriteClient.from_service_account_file
  - google.cloud.bigquery_storage_v1.client.BigQueryWriteClient.from_service_account_info
  - google.cloud.bigquery_storage_v1.client.BigQueryWriteClient.from_service_account_json
  - google.cloud.bigquery_storage_v1.client.BigQueryWriteClient.get_mtls_endpoint_and_cert_source
  - google.cloud.bigquery_storage_v1.client.BigQueryWriteClient.get_write_stream
  - google.cloud.bigquery_storage_v1.client.BigQueryWriteClient.parse_common_billing_account_path
  - google.cloud.bigquery_storage_v1.client.BigQueryWriteClient.parse_common_folder_path
  - google.cloud.bigquery_storage_v1.client.BigQueryWriteClient.parse_common_location_path
  - google.cloud.bigquery_storage_v1.client.BigQueryWriteClient.parse_common_organization_path
  - google.cloud.bigquery_storage_v1.client.BigQueryWriteClient.parse_common_project_path
  - google.cloud.bigquery_storage_v1.client.BigQueryWriteClient.parse_table_path
  - google.cloud.bigquery_storage_v1.client.BigQueryWriteClient.parse_write_stream_path
  - google.cloud.bigquery_storage_v1.client.BigQueryWriteClient.table_path
  - google.cloud.bigquery_storage_v1.client.BigQueryWriteClient.transport
  - google.cloud.bigquery_storage_v1.client.BigQueryWriteClient.write_stream_path
  class: google.cloud.bigquery_storage_v1.client.BigQueryWriteClient
  fullName: google.cloud.bigquery_storage_v1.client.BigQueryWriteClient
  inheritance:
  - inheritance:
    - type: builtins.object
    type: google.cloud.bigquery_storage_v1.services.big_query_write.client.BigQueryWriteClient
  langs:
  - python
  module: google.cloud.bigquery_storage_v1.client
  name: BigQueryWriteClient
  source:
    id: BigQueryWriteClient
    path: google/cloud/bigquery_storage_v1/client.py
    remote:
      branch: main
      path: google/cloud/bigquery_storage_v1/client.py
      repo: git@github.com:googleapis/python-bigquery-storage.git
    startLine: 143
  summary: 'BigQuery Write API.

    The Write API can be used to write data to BigQuery.

    For supplementary information about the Write API, see:

    https://cloud.google.com/bigquery/docs/write-api


    '
  syntax:
    content: 'BigQueryWriteClient(*, credentials: typing.Optional[google.auth.credentials.Credentials]
      = None, transport: typing.Optional[typing.Union[str, google.cloud.bigquery_storage_v1.services.big_query_write.transports.base.BigQueryWriteTransport]]
      = None, client_options: typing.Optional[google.api_core.client_options.ClientOptions]
      = None, client_info: google.api_core.gapic_v1.client_info.ClientInfo = <google.api_core.gapic_v1.client_info.ClientInfo
      object>)'
    parameters: []
  type: class
  uid: google.cloud.bigquery_storage_v1.client.BigQueryWriteClient
- attributes: []
  class: google.cloud.bigquery_storage_v1.client.BigQueryWriteClient
  fullName: google.cloud.bigquery_storage_v1.client.BigQueryWriteClient
  inheritance:
  - inheritance:
    - type: builtins.object
    type: google.cloud.bigquery_storage_v1.services.big_query_write.client.BigQueryWriteClient
  langs:
  - python
  module: google.cloud.bigquery_storage_v1.client
  name: BigQueryWriteClient
  source:
    id: BigQueryWriteClient
    path: google/cloud/bigquery_storage_v1/client.py
    remote:
      branch: main
      path: google/cloud/bigquery_storage_v1/client.py
      repo: git@github.com:googleapis/python-bigquery-storage.git
    startLine: 143
  summary: 'Instantiates the big query write client.

    '
  syntax:
    content: 'BigQueryWriteClient(*, credentials: typing.Optional[google.auth.credentials.Credentials]
      = None, transport: typing.Optional[typing.Union[str, google.cloud.bigquery_storage_v1.services.big_query_write.transports.base.BigQueryWriteTransport]]
      = None, client_options: typing.Optional[google.api_core.client_options.ClientOptions]
      = None, client_info: google.api_core.gapic_v1.client_info.ClientInfo = <google.api_core.gapic_v1.client_info.ClientInfo
      object>)'
    exceptions:
    - description: If mutual TLS transport creation failed for any reason.
      var_type: google.auth.exceptions.MutualTLSChannelError
    parameters:
    - description: The authorization credentials to attach to requests. These credentials
        identify the application to the service; if none are specified, the client
        will attempt to ascertain the credentials from the environment.
      id: credentials
      var_type: Optional[google.auth.credentials.Credentials]
    - description: The transport to use. If set to None, a transport is chosen automatically.
      id: transport
      var_type: Union[str, BigQueryWriteTransport]
    - description: 'Custom options for the client. It won''t take effect if a ``transport``
        instance is provided. (1) The ``api_endpoint`` property can be used to override
        the default endpoint provided by the client. GOOGLE_API_USE_MTLS_ENDPOINT
        environment variable can also be used to override the endpoint: "always" (always
        use the default mTLS endpoint), "never" (always use the default regular endpoint)
        and "auto" (auto switch to the default mTLS endpoint if client certificate
        is present, this is the default value). However, the ``api_endpoint`` property
        takes precedence if provided. (2) If GOOGLE_API_USE_CLIENT_CERTIFICATE environment
        variable is "true", then the ``client_cert_source`` property can be used to
        provide client certificate for mutual TLS transport. If not provided, the
        default SSL client certificate will be used if present. If GOOGLE_API_USE_CLIENT_CERTIFICATE
        is "false" or not set, no client certificate will be used.'
      id: client_options
      var_type: google.api_core.client_options.ClientOptions
    - description: The client info used to send a user-agent string along with API
        requests. If ``None``, then default info will be used. Generally, you only
        need to set this if you're developing your own client library.
      id: client_info
      var_type: google.api_core.gapic_v1.client_info.ClientInfo
  type: method
  uid: google.cloud.bigquery_storage_v1.client.BigQueryWriteClient
- attributes: []
  class: google.cloud.bigquery_storage_v1.client.BigQueryWriteClient
  fullName: google.cloud.bigquery_storage_v1.client.BigQueryWriteClient.__exit__
  langs:
  - python
  module: google.cloud.bigquery_storage_v1.client
  name: __exit__
  source:
    id: __exit__
    path: google/cloud/bigquery_storage_v1/services/big_query_write/client.py
    remote:
      branch: main
      path: google/cloud/bigquery_storage_v1/services/big_query_write/client.py
      repo: git@github.com:googleapis/python-bigquery-storage.git
    startLine: 886
  summary: "Releases underlying transport's resources.\n\n.. warning::\n    ONLY use\
    \ as a context manager if the transport is NOT shared\n    with other clients!\
    \ Exiting the with block will CLOSE the transport\n    and may cause errors in\
    \ other clients!\n\n"
  syntax:
    content: __exit__(type, value, traceback)
    parameters: []
  type: method
  uid: google.cloud.bigquery_storage_v1.client.BigQueryWriteClient.__exit__
- attributes: []
  class: google.cloud.bigquery_storage_v1.client.BigQueryWriteClient
  fullName: google.cloud.bigquery_storage_v1.client.BigQueryWriteClient.append_rows
  langs:
  - python
  module: google.cloud.bigquery_storage_v1.client
  name: append_rows
  source:
    id: append_rows
    path: google/cloud/bigquery_storage_v1/services/big_query_write/client.py
    remote:
      branch: main
      path: google/cloud/bigquery_storage_v1/services/big_query_write/client.py
      repo: git@github.com:googleapis/python-bigquery-storage.git
    startLine: 510
  summary: "Appends data to the given stream.\n\nIf ``offset`` is specified, the ``offset``\
    \ is checked against\nthe end of stream. The server returns ``OUT_OF_RANGE`` in\n\
    ``AppendRowsResponse`` if an attempt is made to append to an\noffset beyond the\
    \ current end of the stream or\n``ALREADY_EXISTS`` if user provides an ``offset``\
    \ that has\nalready been written to. User can retry with adjusted offset\nwithin\
    \ the same RPC connection. If ``offset`` is not specified,\nappend happens at\
    \ the end of the stream.\n\nThe response contains an optional offset at which\
    \ the append\nhappened. No offset information will be returned for appends to\n\
    a default stream.\n\nResponses are received in the same order in which requests\
    \ are\nsent. There will be one response for each successful inserted\nrequest.\
    \ Responses may optionally embed error information if the\noriginating AppendRequest\
    \ was not successfully processed.\n\nThe specifics of when successfully appended\
    \ data is made visible\nto the table are governed by the type of stream:\n\n-\
    \  For COMMITTED streams (which includes the default stream),\n   data is visible\
    \ immediately upon successful append.\n\n-  For BUFFERED streams, data is made\
    \ visible via a subsequent\n   ``FlushRows`` rpc which advances a cursor to a\
    \ newer offset\n   in the stream.\n\n-  For PENDING streams, data is not made\
    \ visible until the\n   stream itself is finalized (via the ``FinalizeWriteStream``\n\
    \   rpc), and the stream is explicitly committed via the\n   ``BatchCommitWriteStreams``\
    \ rpc.\n"
  syntax:
    content: "append_rows(\n    requests: typing.Optional[\n        typing.Iterator[\n\
      \            google.cloud.bigquery_storage_v1.types.storage.AppendRowsRequest\n\
      \        ]\n    ] = None,\n    *,\n    retry: typing.Union[\n        google.api_core.retry.Retry,\
      \ google.api_core.gapic_v1.method._MethodDefault\n    ] = _MethodDefault._DEFAULT_VALUE,\n\
      \    timeout: typing.Optional[float] = None,\n    metadata: typing.Sequence[typing.Tuple[str,\
      \ str]] = ()\n)"
    parameters:
    - defaultValue: None
      description: The request object iterator. Request message for `AppendRows`.
        Due to the nature of AppendRows being a bidirectional streaming RPC, certain
        parts of the AppendRowsRequest need only be specified for the first request
        sent each time the gRPC network connection is opened/reopened.
      id: requests
      var_type: Iterator[<xref uid="google.cloud.bigquery_storage_v1.types.AppendRowsRequest">google.cloud.bigquery_storage_v1.types.AppendRowsRequest</xref>]
    - description: Designation of what errors, if any, should be retried.
      id: retry
      var_type: google.api_core.retry.Retry
    - description: The timeout for this request.
      id: timeout
      var_type: float
    - description: Strings which should be sent along with the request as metadata.
      id: metadata
      var_type: Sequence[Tuple[str, str]]
    returns:
    - description: Response message for AppendRows.
      var_type: Iterable[<xref uid="google.cloud.bigquery_storage_v1.types.AppendRowsResponse">google.cloud.bigquery_storage_v1.types.AppendRowsResponse</xref>]
  type: method
  uid: google.cloud.bigquery_storage_v1.client.BigQueryWriteClient.append_rows
- attributes: []
  class: google.cloud.bigquery_storage_v1.client.BigQueryWriteClient
  fullName: google.cloud.bigquery_storage_v1.client.BigQueryWriteClient.batch_commit_write_streams
  langs:
  - python
  module: google.cloud.bigquery_storage_v1.client
  name: batch_commit_write_streams
  source:
    id: batch_commit_write_streams
    path: google/cloud/bigquery_storage_v1/services/big_query_write/client.py
    remote:
      branch: main
      path: google/cloud/bigquery_storage_v1/services/big_query_write/client.py
      repo: git@github.com:googleapis/python-bigquery-storage.git
    startLine: 727
  summary: 'Atomically commits a group of ``PENDING`` streams that belong to

    the same ``parent`` table.


    Streams must be finalized before commit and cannot be committed

    multiple times. Once a stream is committed, data in the stream

    becomes available for read operations.

    '
  syntax:
    content: "batch_commit_write_streams(\n    request: typing.Optional[\n       \
      \ typing.Union[\n            google.cloud.bigquery_storage_v1.types.storage.BatchCommitWriteStreamsRequest,\n\
      \            dict,\n        ]\n    ] = None,\n    *,\n    parent: typing.Optional[str]\
      \ = None,\n    retry: typing.Union[\n        google.api_core.retry.Retry, google.api_core.gapic_v1.method._MethodDefault\n\
      \    ] = _MethodDefault._DEFAULT_VALUE,\n    timeout: typing.Optional[float]\
      \ = None,\n    metadata: typing.Sequence[typing.Tuple[str, str]] = ()\n)"
    parameters:
    - defaultValue: None
      description: The request object. Request message for `BatchCommitWriteStreams`.
      id: request
      var_type: Union[<xref uid="google.cloud.bigquery_storage_v1.types.BatchCommitWriteStreamsRequest">google.cloud.bigquery_storage_v1.types.BatchCommitWriteStreamsRequest</xref>,
        dict]
    - description: Required. Parent table that all the streams should belong to, in
        the form of ``projects/{project}/datasets/{dataset}/tables/{table}``. This
        corresponds to the ``parent`` field on the ``request`` instance; if ``request``
        is provided, this should not be set.
      id: parent
      var_type: str
    - description: Designation of what errors, if any, should be retried.
      id: retry
      var_type: google.api_core.retry.Retry
    - description: The timeout for this request.
      id: timeout
      var_type: float
    - description: Strings which should be sent along with the request as metadata.
      id: metadata
      var_type: Sequence[Tuple[str, str]]
    returns:
    - description: Response message for BatchCommitWriteStreams.
      var_type: <xref uid="google.cloud.bigquery_storage_v1.types.BatchCommitWriteStreamsResponse">google.cloud.bigquery_storage_v1.types.BatchCommitWriteStreamsResponse</xref>
  type: method
  uid: google.cloud.bigquery_storage_v1.client.BigQueryWriteClient.batch_commit_write_streams
- attributes: []
  class: google.cloud.bigquery_storage_v1.client.BigQueryWriteClient
  fullName: google.cloud.bigquery_storage_v1.client.BigQueryWriteClient.common_billing_account_path
  langs:
  - python
  module: google.cloud.bigquery_storage_v1.client
  name: common_billing_account_path
  source:
    id: common_billing_account_path
    path: google/cloud/bigquery_storage_v1/services/big_query_write/client.py
    remote:
      branch: main
      path: google/cloud/bigquery_storage_v1/services/big_query_write/client.py
      repo: git@github.com:googleapis/python-bigquery-storage.git
    startLine: 199
  summary: 'Returns a fully-qualified billing_account string.


    '
  syntax:
    content: 'common_billing_account_path(billing_account: str)'
    parameters: []
  type: method
  uid: google.cloud.bigquery_storage_v1.client.BigQueryWriteClient.common_billing_account_path
- attributes: []
  class: google.cloud.bigquery_storage_v1.client.BigQueryWriteClient
  fullName: google.cloud.bigquery_storage_v1.client.BigQueryWriteClient.common_folder_path
  langs:
  - python
  module: google.cloud.bigquery_storage_v1.client
  name: common_folder_path
  source:
    id: common_folder_path
    path: google/cloud/bigquery_storage_v1/services/big_query_write/client.py
    remote:
      branch: main
      path: google/cloud/bigquery_storage_v1/services/big_query_write/client.py
      repo: git@github.com:googleapis/python-bigquery-storage.git
    startLine: 212
  summary: 'Returns a fully-qualified folder string.


    '
  syntax:
    content: 'common_folder_path(folder: str)'
    parameters: []
  type: method
  uid: google.cloud.bigquery_storage_v1.client.BigQueryWriteClient.common_folder_path
- attributes: []
  class: google.cloud.bigquery_storage_v1.client.BigQueryWriteClient
  fullName: google.cloud.bigquery_storage_v1.client.BigQueryWriteClient.common_location_path
  langs:
  - python
  module: google.cloud.bigquery_storage_v1.client
  name: common_location_path
  source:
    id: common_location_path
    path: google/cloud/bigquery_storage_v1/services/big_query_write/client.py
    remote:
      branch: main
      path: google/cloud/bigquery_storage_v1/services/big_query_write/client.py
      repo: git@github.com:googleapis/python-bigquery-storage.git
    startLine: 245
  summary: 'Returns a fully-qualified location string.


    '
  syntax:
    content: 'common_location_path(project: str, location: str)'
    parameters: []
  type: method
  uid: google.cloud.bigquery_storage_v1.client.BigQueryWriteClient.common_location_path
- attributes: []
  class: google.cloud.bigquery_storage_v1.client.BigQueryWriteClient
  fullName: google.cloud.bigquery_storage_v1.client.BigQueryWriteClient.common_organization_path
  langs:
  - python
  module: google.cloud.bigquery_storage_v1.client
  name: common_organization_path
  source:
    id: common_organization_path
    path: google/cloud/bigquery_storage_v1/services/big_query_write/client.py
    remote:
      branch: main
      path: google/cloud/bigquery_storage_v1/services/big_query_write/client.py
      repo: git@github.com:googleapis/python-bigquery-storage.git
    startLine: 223
  summary: 'Returns a fully-qualified organization string.


    '
  syntax:
    content: 'common_organization_path(organization: str)'
    parameters: []
  type: method
  uid: google.cloud.bigquery_storage_v1.client.BigQueryWriteClient.common_organization_path
- attributes: []
  class: google.cloud.bigquery_storage_v1.client.BigQueryWriteClient
  fullName: google.cloud.bigquery_storage_v1.client.BigQueryWriteClient.common_project_path
  langs:
  - python
  module: google.cloud.bigquery_storage_v1.client
  name: common_project_path
  source:
    id: common_project_path
    path: google/cloud/bigquery_storage_v1/services/big_query_write/client.py
    remote:
      branch: main
      path: google/cloud/bigquery_storage_v1/services/big_query_write/client.py
      repo: git@github.com:googleapis/python-bigquery-storage.git
    startLine: 234
  summary: 'Returns a fully-qualified project string.


    '
  syntax:
    content: 'common_project_path(project: str)'
    parameters: []
  type: method
  uid: google.cloud.bigquery_storage_v1.client.BigQueryWriteClient.common_project_path
- attributes: []
  class: google.cloud.bigquery_storage_v1.client.BigQueryWriteClient
  fullName: google.cloud.bigquery_storage_v1.client.BigQueryWriteClient.create_write_stream
  langs:
  - python
  module: google.cloud.bigquery_storage_v1.client
  name: create_write_stream
  source:
    id: create_write_stream
    path: google/cloud/bigquery_storage_v1/services/big_query_write/client.py
    remote:
      branch: main
      path: google/cloud/bigquery_storage_v1/services/big_query_write/client.py
      repo: git@github.com:googleapis/python-bigquery-storage.git
    startLine: 423
  summary: 'Creates a write stream to the given table. Additionally, every

    table has a special stream named ''_default'' to which data can be

    written. This stream doesn''t need to be created using

    CreateWriteStream. It is a stream that can be used

    simultaneously by any number of clients. Data written to this

    stream is considered committed as soon as an acknowledgement is

    received.

    '
  syntax:
    content: "create_write_stream(\n    request: typing.Optional[\n        typing.Union[\n\
      \            google.cloud.bigquery_storage_v1.types.storage.CreateWriteStreamRequest,\n\
      \            dict,\n        ]\n    ] = None,\n    *,\n    parent: typing.Optional[str]\
      \ = None,\n    write_stream: typing.Optional[\n        google.cloud.bigquery_storage_v1.types.stream.WriteStream\n\
      \    ] = None,\n    retry: typing.Union[\n        google.api_core.retry.Retry,\
      \ google.api_core.gapic_v1.method._MethodDefault\n    ] = _MethodDefault._DEFAULT_VALUE,\n\
      \    timeout: typing.Optional[float] = None,\n    metadata: typing.Sequence[typing.Tuple[str,\
      \ str]] = ()\n)"
    parameters:
    - defaultValue: None
      description: The request object. Request message for `CreateWriteStream`.
      id: request
      var_type: Union[<xref uid="google.cloud.bigquery_storage_v1.types.CreateWriteStreamRequest">google.cloud.bigquery_storage_v1.types.CreateWriteStreamRequest</xref>,
        dict]
    - description: Required. Reference to the table to which the stream belongs, in
        the format of ``projects/{project}/datasets/{dataset}/tables/{table}``. This
        corresponds to the ``parent`` field on the ``request`` instance; if ``request``
        is provided, this should not be set.
      id: parent
      var_type: str
    - description: Required. Stream to be created. This corresponds to the ``write_stream``
        field on the ``request`` instance; if ``request`` is provided, this should
        not be set.
      id: write_stream
      var_type: <xref uid="google.cloud.bigquery_storage_v1.types.WriteStream">google.cloud.bigquery_storage_v1.types.WriteStream</xref>
    - description: Designation of what errors, if any, should be retried.
      id: retry
      var_type: google.api_core.retry.Retry
    - description: The timeout for this request.
      id: timeout
      var_type: float
    - description: Strings which should be sent along with the request as metadata.
      id: metadata
      var_type: Sequence[Tuple[str, str]]
    returns:
    - description: Information about a single stream that gets data inside the storage
        system.
      var_type: <xref uid="google.cloud.bigquery_storage_v1.types.WriteStream">google.cloud.bigquery_storage_v1.types.WriteStream</xref>
  type: method
  uid: google.cloud.bigquery_storage_v1.client.BigQueryWriteClient.create_write_stream
- attributes: []
  class: google.cloud.bigquery_storage_v1.client.BigQueryWriteClient
  fullName: google.cloud.bigquery_storage_v1.client.BigQueryWriteClient.finalize_write_stream
  langs:
  - python
  module: google.cloud.bigquery_storage_v1.client
  name: finalize_write_stream
  source:
    id: finalize_write_stream
    path: google/cloud/bigquery_storage_v1/services/big_query_write/client.py
    remote:
      branch: main
      path: google/cloud/bigquery_storage_v1/services/big_query_write/client.py
      repo: git@github.com:googleapis/python-bigquery-storage.git
    startLine: 657
  summary: 'Finalize a write stream so that no new data can be appended to

    the stream. Finalize is not supported on the ''_default'' stream.

    '
  syntax:
    content: "finalize_write_stream(\n    request: typing.Optional[\n        typing.Union[\n\
      \            google.cloud.bigquery_storage_v1.types.storage.FinalizeWriteStreamRequest,\n\
      \            dict,\n        ]\n    ] = None,\n    *,\n    name: typing.Optional[str]\
      \ = None,\n    retry: typing.Union[\n        google.api_core.retry.Retry, google.api_core.gapic_v1.method._MethodDefault\n\
      \    ] = _MethodDefault._DEFAULT_VALUE,\n    timeout: typing.Optional[float]\
      \ = None,\n    metadata: typing.Sequence[typing.Tuple[str, str]] = ()\n)"
    parameters:
    - defaultValue: None
      description: The request object. Request message for invoking `FinalizeWriteStream`.
      id: request
      var_type: Union[<xref uid="google.cloud.bigquery_storage_v1.types.FinalizeWriteStreamRequest">google.cloud.bigquery_storage_v1.types.FinalizeWriteStreamRequest</xref>,
        dict]
    - description: Required. Name of the stream to finalize, in the form of ``projects/{project}/datasets/{dataset}/tables/{table}/streams/{stream}``.
        This corresponds to the ``name`` field on the ``request`` instance; if ``request``
        is provided, this should not be set.
      id: name
      var_type: str
    - description: Designation of what errors, if any, should be retried.
      id: retry
      var_type: google.api_core.retry.Retry
    - description: The timeout for this request.
      id: timeout
      var_type: float
    - description: Strings which should be sent along with the request as metadata.
      id: metadata
      var_type: Sequence[Tuple[str, str]]
    returns:
    - description: Response message for FinalizeWriteStream.
      var_type: <xref uid="google.cloud.bigquery_storage_v1.types.FinalizeWriteStreamResponse">google.cloud.bigquery_storage_v1.types.FinalizeWriteStreamResponse</xref>
  type: method
  uid: google.cloud.bigquery_storage_v1.client.BigQueryWriteClient.finalize_write_stream
- attributes: []
  class: google.cloud.bigquery_storage_v1.client.BigQueryWriteClient
  fullName: google.cloud.bigquery_storage_v1.client.BigQueryWriteClient.flush_rows
  langs:
  - python
  module: google.cloud.bigquery_storage_v1.client
  name: flush_rows
  source:
    id: flush_rows
    path: google/cloud/bigquery_storage_v1/services/big_query_write/client.py
    remote:
      branch: main
      path: google/cloud/bigquery_storage_v1/services/big_query_write/client.py
      repo: git@github.com:googleapis/python-bigquery-storage.git
    startLine: 804
  summary: 'Flushes rows to a BUFFERED stream.


    If users are appending rows to BUFFERED stream, flush operation

    is required in order for the rows to become available for

    reading. A Flush operation flushes up to any previously flushed

    offset in a BUFFERED stream, to the offset specified in the

    request.


    Flush is not supported on the \_default stream, since it is not

    BUFFERED.

    '
  syntax:
    content: "flush_rows(\n    request: typing.Optional[\n        typing.Union[\n\
      \            google.cloud.bigquery_storage_v1.types.storage.FlushRowsRequest,\
      \ dict\n        ]\n    ] = None,\n    *,\n    write_stream: typing.Optional[str]\
      \ = None,\n    retry: typing.Union[\n        google.api_core.retry.Retry, google.api_core.gapic_v1.method._MethodDefault\n\
      \    ] = _MethodDefault._DEFAULT_VALUE,\n    timeout: typing.Optional[float]\
      \ = None,\n    metadata: typing.Sequence[typing.Tuple[str, str]] = ()\n)"
    parameters:
    - defaultValue: None
      description: The request object. Request message for `FlushRows`.
      id: request
      var_type: Union[<xref uid="google.cloud.bigquery_storage_v1.types.FlushRowsRequest">google.cloud.bigquery_storage_v1.types.FlushRowsRequest</xref>,
        dict]
    - description: Required. The stream that is the target of the flush operation.
        This corresponds to the ``write_stream`` field on the ``request`` instance;
        if ``request`` is provided, this should not be set.
      id: write_stream
      var_type: str
    - description: Designation of what errors, if any, should be retried.
      id: retry
      var_type: google.api_core.retry.Retry
    - description: The timeout for this request.
      id: timeout
      var_type: float
    - description: Strings which should be sent along with the request as metadata.
      id: metadata
      var_type: Sequence[Tuple[str, str]]
    returns:
    - description: Respond message for FlushRows.
      var_type: <xref uid="google.cloud.bigquery_storage_v1.types.FlushRowsResponse">google.cloud.bigquery_storage_v1.types.FlushRowsResponse</xref>
  type: method
  uid: google.cloud.bigquery_storage_v1.client.BigQueryWriteClient.flush_rows
- attributes: []
  class: google.cloud.bigquery_storage_v1.client.BigQueryWriteClient
  fullName: google.cloud.bigquery_storage_v1.client.BigQueryWriteClient.from_service_account_file
  langs:
  - python
  module: google.cloud.bigquery_storage_v1.client
  name: from_service_account_file
  source:
    id: from_service_account_file
    path: google/cloud/bigquery_storage_v1/services/big_query_write/client.py
    remote:
      branch: main
      path: google/cloud/bigquery_storage_v1/services/big_query_write/client.py
      repo: git@github.com:googleapis/python-bigquery-storage.git
    startLine: 137
  summary: "Creates an instance of this client using the provided credentials\n  \
    \  file.\n"
  syntax:
    content: 'from_service_account_file(filename: str, *args, **kwargs)'
    parameters:
    - description: The path to the service account private key json file.
      id: filename
      var_type: str
    returns:
    - description: The constructed client.
      var_type: BigQueryWriteClient
  type: method
  uid: google.cloud.bigquery_storage_v1.client.BigQueryWriteClient.from_service_account_file
- attributes: []
  class: google.cloud.bigquery_storage_v1.client.BigQueryWriteClient
  fullName: google.cloud.bigquery_storage_v1.client.BigQueryWriteClient.from_service_account_info
  langs:
  - python
  module: google.cloud.bigquery_storage_v1.client
  name: from_service_account_info
  source:
    id: from_service_account_info
    path: google/cloud/bigquery_storage_v1/services/big_query_write/client.py
    remote:
      branch: main
      path: google/cloud/bigquery_storage_v1/services/big_query_write/client.py
      repo: git@github.com:googleapis/python-bigquery-storage.git
    startLine: 120
  summary: "Creates an instance of this client using the provided credentials\n  \
    \  info.\n"
  syntax:
    content: 'from_service_account_info(info: dict, *args, **kwargs)'
    parameters:
    - description: The service account private key info.
      id: info
      var_type: dict
    returns:
    - description: The constructed client.
      var_type: BigQueryWriteClient
  type: method
  uid: google.cloud.bigquery_storage_v1.client.BigQueryWriteClient.from_service_account_info
- attributes: []
  class: google.cloud.bigquery_storage_v1.client.BigQueryWriteClient
  fullName: google.cloud.bigquery_storage_v1.client.BigQueryWriteClient.from_service_account_json
  langs:
  - python
  module: google.cloud.bigquery_storage_v1.client
  name: from_service_account_json
  source:
    id: from_service_account_json
    path: google/cloud/bigquery_storage_v1/services/big_query_write/client.py
    remote:
      branch: main
      path: google/cloud/bigquery_storage_v1/services/big_query_write/client.py
      repo: git@github.com:googleapis/python-bigquery-storage.git
    startLine: 137
  summary: "Creates an instance of this client using the provided credentials\n  \
    \  file.\n"
  syntax:
    content: 'from_service_account_json(filename: str, *args, **kwargs)'
    parameters:
    - description: The path to the service account private key json file.
      id: filename
      var_type: str
    returns:
    - description: The constructed client.
      var_type: BigQueryWriteClient
  type: method
  uid: google.cloud.bigquery_storage_v1.client.BigQueryWriteClient.from_service_account_json
- attributes: []
  class: google.cloud.bigquery_storage_v1.client.BigQueryWriteClient
  fullName: google.cloud.bigquery_storage_v1.client.BigQueryWriteClient.get_mtls_endpoint_and_cert_source
  langs:
  - python
  module: google.cloud.bigquery_storage_v1.client
  name: get_mtls_endpoint_and_cert_source
  source:
    id: get_mtls_endpoint_and_cert_source
    path: google/cloud/bigquery_storage_v1/services/big_query_write/client.py
    remote:
      branch: main
      path: google/cloud/bigquery_storage_v1/services/big_query_write/client.py
      repo: git@github.com:googleapis/python-bigquery-storage.git
    startLine: 258
  summary: 'Return the API endpoint and client cert source for mutual TLS.


    The client cert source is determined in the following order:

    (1) if `GOOGLE_API_USE_CLIENT_CERTIFICATE` environment variable is not "true",
    the

    client cert source is None.

    (2) if `client_options.client_cert_source` is provided, use the provided one;
    if the

    default client cert source exists, use the default one; otherwise the client cert

    source is None.


    The API endpoint is determined in the following order:

    (1) if `client_options.api_endpoint` if provided, use the provided one.

    (2) if `GOOGLE_API_USE_CLIENT_CERTIFICATE` environment variable is "always", use
    the

    default mTLS endpoint; if the environment variabel is "never", use the default
    API

    endpoint; otherwise if client cert source exists, use the default mTLS endpoint,
    otherwise

    use the default API endpoint.


    More details can be found at https://google.aip.dev/auth/4114.

    '
  syntax:
    content: "get_mtls_endpoint_and_cert_source(\n    client_options: typing.Optional[\n\
      \        google.api_core.client_options.ClientOptions\n    ] = None,\n)"
    exceptions:
    - description: If any errors happen.
      var_type: google.auth.exceptions.MutualTLSChannelError
    parameters:
    - defaultValue: None
      description: Custom options for the client. Only the `api_endpoint` and `client_cert_source`
        properties may be used in this method.
      id: client_options
      var_type: google.api_core.client_options.ClientOptions
    returns:
    - description: returns the API endpoint and the client cert source to use.
      var_type: Tuple[str, Callable[[], Tuple[bytes, bytes]]]
  type: method
  uid: google.cloud.bigquery_storage_v1.client.BigQueryWriteClient.get_mtls_endpoint_and_cert_source
- attributes: []
  class: google.cloud.bigquery_storage_v1.client.BigQueryWriteClient
  fullName: google.cloud.bigquery_storage_v1.client.BigQueryWriteClient.get_write_stream
  langs:
  - python
  module: google.cloud.bigquery_storage_v1.client
  name: get_write_stream
  source:
    id: get_write_stream
    path: google/cloud/bigquery_storage_v1/services/big_query_write/client.py
    remote:
      branch: main
      path: google/cloud/bigquery_storage_v1/services/big_query_write/client.py
      repo: git@github.com:googleapis/python-bigquery-storage.git
    startLine: 585
  summary: 'Gets information about a write stream.

    '
  syntax:
    content: "get_write_stream(\n    request: typing.Optional[\n        typing.Union[\n\
      \            google.cloud.bigquery_storage_v1.types.storage.GetWriteStreamRequest,\
      \ dict\n        ]\n    ] = None,\n    *,\n    name: typing.Optional[str] = None,\n\
      \    retry: typing.Union[\n        google.api_core.retry.Retry, google.api_core.gapic_v1.method._MethodDefault\n\
      \    ] = _MethodDefault._DEFAULT_VALUE,\n    timeout: typing.Optional[float]\
      \ = None,\n    metadata: typing.Sequence[typing.Tuple[str, str]] = ()\n)"
    parameters:
    - defaultValue: None
      description: The request object. Request message for `GetWriteStreamRequest`.
      id: request
      var_type: Union[<xref uid="google.cloud.bigquery_storage_v1.types.GetWriteStreamRequest">google.cloud.bigquery_storage_v1.types.GetWriteStreamRequest</xref>,
        dict]
    - description: Required. Name of the stream to get, in the form of ``projects/{project}/datasets/{dataset}/tables/{table}/streams/{stream}``.
        This corresponds to the ``name`` field on the ``request`` instance; if ``request``
        is provided, this should not be set.
      id: name
      var_type: str
    - description: Designation of what errors, if any, should be retried.
      id: retry
      var_type: google.api_core.retry.Retry
    - description: The timeout for this request.
      id: timeout
      var_type: float
    - description: Strings which should be sent along with the request as metadata.
      id: metadata
      var_type: Sequence[Tuple[str, str]]
    returns:
    - description: Information about a single stream that gets data inside the storage
        system.
      var_type: <xref uid="google.cloud.bigquery_storage_v1.types.WriteStream">google.cloud.bigquery_storage_v1.types.WriteStream</xref>
  type: method
  uid: google.cloud.bigquery_storage_v1.client.BigQueryWriteClient.get_write_stream
- attributes: []
  class: google.cloud.bigquery_storage_v1.client.BigQueryWriteClient
  fullName: google.cloud.bigquery_storage_v1.client.BigQueryWriteClient.parse_common_billing_account_path
  langs:
  - python
  module: google.cloud.bigquery_storage_v1.client
  name: parse_common_billing_account_path
  source:
    id: parse_common_billing_account_path
    path: google/cloud/bigquery_storage_v1/services/big_query_write/client.py
    remote:
      branch: main
      path: google/cloud/bigquery_storage_v1/services/big_query_write/client.py
      repo: git@github.com:googleapis/python-bigquery-storage.git
    startLine: 206
  summary: 'Parse a billing_account path into its component segments.


    '
  syntax:
    content: 'parse_common_billing_account_path(path: str)'
    parameters: []
  type: method
  uid: google.cloud.bigquery_storage_v1.client.BigQueryWriteClient.parse_common_billing_account_path
- attributes: []
  class: google.cloud.bigquery_storage_v1.client.BigQueryWriteClient
  fullName: google.cloud.bigquery_storage_v1.client.BigQueryWriteClient.parse_common_folder_path
  langs:
  - python
  module: google.cloud.bigquery_storage_v1.client
  name: parse_common_folder_path
  source:
    id: parse_common_folder_path
    path: google/cloud/bigquery_storage_v1/services/big_query_write/client.py
    remote:
      branch: main
      path: google/cloud/bigquery_storage_v1/services/big_query_write/client.py
      repo: git@github.com:googleapis/python-bigquery-storage.git
    startLine: 217
  summary: 'Parse a folder path into its component segments.


    '
  syntax:
    content: 'parse_common_folder_path(path: str)'
    parameters: []
  type: method
  uid: google.cloud.bigquery_storage_v1.client.BigQueryWriteClient.parse_common_folder_path
- attributes: []
  class: google.cloud.bigquery_storage_v1.client.BigQueryWriteClient
  fullName: google.cloud.bigquery_storage_v1.client.BigQueryWriteClient.parse_common_location_path
  langs:
  - python
  module: google.cloud.bigquery_storage_v1.client
  name: parse_common_location_path
  source:
    id: parse_common_location_path
    path: google/cloud/bigquery_storage_v1/services/big_query_write/client.py
    remote:
      branch: main
      path: google/cloud/bigquery_storage_v1/services/big_query_write/client.py
      repo: git@github.com:googleapis/python-bigquery-storage.git
    startLine: 252
  summary: 'Parse a location path into its component segments.


    '
  syntax:
    content: 'parse_common_location_path(path: str)'
    parameters: []
  type: method
  uid: google.cloud.bigquery_storage_v1.client.BigQueryWriteClient.parse_common_location_path
- attributes: []
  class: google.cloud.bigquery_storage_v1.client.BigQueryWriteClient
  fullName: google.cloud.bigquery_storage_v1.client.BigQueryWriteClient.parse_common_organization_path
  langs:
  - python
  module: google.cloud.bigquery_storage_v1.client
  name: parse_common_organization_path
  source:
    id: parse_common_organization_path
    path: google/cloud/bigquery_storage_v1/services/big_query_write/client.py
    remote:
      branch: main
      path: google/cloud/bigquery_storage_v1/services/big_query_write/client.py
      repo: git@github.com:googleapis/python-bigquery-storage.git
    startLine: 228
  summary: 'Parse a organization path into its component segments.


    '
  syntax:
    content: 'parse_common_organization_path(path: str)'
    parameters: []
  type: method
  uid: google.cloud.bigquery_storage_v1.client.BigQueryWriteClient.parse_common_organization_path
- attributes: []
  class: google.cloud.bigquery_storage_v1.client.BigQueryWriteClient
  fullName: google.cloud.bigquery_storage_v1.client.BigQueryWriteClient.parse_common_project_path
  langs:
  - python
  module: google.cloud.bigquery_storage_v1.client
  name: parse_common_project_path
  source:
    id: parse_common_project_path
    path: google/cloud/bigquery_storage_v1/services/big_query_write/client.py
    remote:
      branch: main
      path: google/cloud/bigquery_storage_v1/services/big_query_write/client.py
      repo: git@github.com:googleapis/python-bigquery-storage.git
    startLine: 239
  summary: 'Parse a project path into its component segments.


    '
  syntax:
    content: 'parse_common_project_path(path: str)'
    parameters: []
  type: method
  uid: google.cloud.bigquery_storage_v1.client.BigQueryWriteClient.parse_common_project_path
- attributes: []
  class: google.cloud.bigquery_storage_v1.client.BigQueryWriteClient
  fullName: google.cloud.bigquery_storage_v1.client.BigQueryWriteClient.parse_table_path
  langs:
  - python
  module: google.cloud.bigquery_storage_v1.client
  name: parse_table_path
  source:
    id: parse_table_path
    path: google/cloud/bigquery_storage_v1/services/big_query_write/client.py
    remote:
      branch: main
      path: google/cloud/bigquery_storage_v1/services/big_query_write/client.py
      repo: git@github.com:googleapis/python-bigquery-storage.git
    startLine: 174
  summary: 'Parses a table path into its component segments.


    '
  syntax:
    content: 'parse_table_path(path: str)'
    parameters: []
  type: method
  uid: google.cloud.bigquery_storage_v1.client.BigQueryWriteClient.parse_table_path
- attributes: []
  class: google.cloud.bigquery_storage_v1.client.BigQueryWriteClient
  fullName: google.cloud.bigquery_storage_v1.client.BigQueryWriteClient.parse_write_stream_path
  langs:
  - python
  module: google.cloud.bigquery_storage_v1.client
  name: parse_write_stream_path
  source:
    id: parse_write_stream_path
    path: google/cloud/bigquery_storage_v1/services/big_query_write/client.py
    remote:
      branch: main
      path: google/cloud/bigquery_storage_v1/services/big_query_write/client.py
      repo: git@github.com:googleapis/python-bigquery-storage.git
    startLine: 190
  summary: 'Parses a write_stream path into its component segments.


    '
  syntax:
    content: 'parse_write_stream_path(path: str)'
    parameters: []
  type: method
  uid: google.cloud.bigquery_storage_v1.client.BigQueryWriteClient.parse_write_stream_path
- attributes: []
  class: google.cloud.bigquery_storage_v1.client.BigQueryWriteClient
  fullName: google.cloud.bigquery_storage_v1.client.BigQueryWriteClient.table_path
  langs:
  - python
  module: google.cloud.bigquery_storage_v1.client
  name: table_path
  source:
    id: table_path
    path: google/cloud/bigquery_storage_v1/services/big_query_write/client.py
    remote:
      branch: main
      path: google/cloud/bigquery_storage_v1/services/big_query_write/client.py
      repo: git@github.com:googleapis/python-bigquery-storage.git
    startLine: 167
  summary: 'Returns a fully-qualified table string.


    '
  syntax:
    content: 'table_path(project: str, dataset: str, table: str)'
    parameters: []
  type: method
  uid: google.cloud.bigquery_storage_v1.client.BigQueryWriteClient.table_path
- &id001
  attributes: []
  class: google.cloud.bigquery_storage_v1.client.BigQueryWriteClient
  fullName: google.cloud.bigquery_storage_v1.client.BigQueryWriteClient.transport
  langs:
  - python
  module: google.cloud.bigquery_storage_v1.client
  name: transport
  source:
    id: transport
    path: null
    remote:
      branch: main
      path: null
      repo: git@github.com:googleapis/python-bigquery-storage.git
    startLine: null
  summary: 'Returns the transport used by the client instance.

    '
  syntax:
    returns:
    - description: The transport used by the client instance.
      var_type: BigQueryWriteTransport
  type: property
  uid: google.cloud.bigquery_storage_v1.client.BigQueryWriteClient.transport
- *id001
- attributes: []
  class: google.cloud.bigquery_storage_v1.client.BigQueryWriteClient
  fullName: google.cloud.bigquery_storage_v1.client.BigQueryWriteClient.write_stream_path
  langs:
  - python
  module: google.cloud.bigquery_storage_v1.client
  name: write_stream_path
  source:
    id: write_stream_path
    path: google/cloud/bigquery_storage_v1/services/big_query_write/client.py
    remote:
      branch: main
      path: google/cloud/bigquery_storage_v1/services/big_query_write/client.py
      repo: git@github.com:googleapis/python-bigquery-storage.git
    startLine: 183
  summary: 'Returns a fully-qualified write_stream string.


    '
  syntax:
    content: 'write_stream_path(project: str, dataset: str, table: str, stream: str)'
    parameters: []
  type: method
  uid: google.cloud.bigquery_storage_v1.client.BigQueryWriteClient.write_stream_path
references:
- fullName: google.cloud.bigquery_storage_v1.client.BigQueryWriteClient
  isExternal: false
  name: BigQueryWriteClient
  parent: google.cloud.bigquery_storage_v1.client.BigQueryWriteClient
  uid: google.cloud.bigquery_storage_v1.client.BigQueryWriteClient
- fullName: google.cloud.bigquery_storage_v1.client.BigQueryWriteClient.__exit__
  isExternal: false
  name: __exit__
  parent: google.cloud.bigquery_storage_v1.client.BigQueryWriteClient
  uid: google.cloud.bigquery_storage_v1.client.BigQueryWriteClient.__exit__
- fullName: google.cloud.bigquery_storage_v1.client.BigQueryWriteClient.append_rows
  isExternal: false
  name: append_rows
  parent: google.cloud.bigquery_storage_v1.client.BigQueryWriteClient
  uid: google.cloud.bigquery_storage_v1.client.BigQueryWriteClient.append_rows
- fullName: google.cloud.bigquery_storage_v1.client.BigQueryWriteClient.batch_commit_write_streams
  isExternal: false
  name: batch_commit_write_streams
  parent: google.cloud.bigquery_storage_v1.client.BigQueryWriteClient
  uid: google.cloud.bigquery_storage_v1.client.BigQueryWriteClient.batch_commit_write_streams
- fullName: google.cloud.bigquery_storage_v1.client.BigQueryWriteClient.common_billing_account_path
  isExternal: false
  name: common_billing_account_path
  parent: google.cloud.bigquery_storage_v1.client.BigQueryWriteClient
  uid: google.cloud.bigquery_storage_v1.client.BigQueryWriteClient.common_billing_account_path
- fullName: google.cloud.bigquery_storage_v1.client.BigQueryWriteClient.common_folder_path
  isExternal: false
  name: common_folder_path
  parent: google.cloud.bigquery_storage_v1.client.BigQueryWriteClient
  uid: google.cloud.bigquery_storage_v1.client.BigQueryWriteClient.common_folder_path
- fullName: google.cloud.bigquery_storage_v1.client.BigQueryWriteClient.common_location_path
  isExternal: false
  name: common_location_path
  parent: google.cloud.bigquery_storage_v1.client.BigQueryWriteClient
  uid: google.cloud.bigquery_storage_v1.client.BigQueryWriteClient.common_location_path
- fullName: google.cloud.bigquery_storage_v1.client.BigQueryWriteClient.common_organization_path
  isExternal: false
  name: common_organization_path
  parent: google.cloud.bigquery_storage_v1.client.BigQueryWriteClient
  uid: google.cloud.bigquery_storage_v1.client.BigQueryWriteClient.common_organization_path
- fullName: google.cloud.bigquery_storage_v1.client.BigQueryWriteClient.common_project_path
  isExternal: false
  name: common_project_path
  parent: google.cloud.bigquery_storage_v1.client.BigQueryWriteClient
  uid: google.cloud.bigquery_storage_v1.client.BigQueryWriteClient.common_project_path
- fullName: google.cloud.bigquery_storage_v1.client.BigQueryWriteClient.create_write_stream
  isExternal: false
  name: create_write_stream
  parent: google.cloud.bigquery_storage_v1.client.BigQueryWriteClient
  uid: google.cloud.bigquery_storage_v1.client.BigQueryWriteClient.create_write_stream
- fullName: google.cloud.bigquery_storage_v1.client.BigQueryWriteClient.finalize_write_stream
  isExternal: false
  name: finalize_write_stream
  parent: google.cloud.bigquery_storage_v1.client.BigQueryWriteClient
  uid: google.cloud.bigquery_storage_v1.client.BigQueryWriteClient.finalize_write_stream
- fullName: google.cloud.bigquery_storage_v1.client.BigQueryWriteClient.flush_rows
  isExternal: false
  name: flush_rows
  parent: google.cloud.bigquery_storage_v1.client.BigQueryWriteClient
  uid: google.cloud.bigquery_storage_v1.client.BigQueryWriteClient.flush_rows
- fullName: google.cloud.bigquery_storage_v1.client.BigQueryWriteClient.from_service_account_file
  isExternal: false
  name: from_service_account_file
  parent: google.cloud.bigquery_storage_v1.client.BigQueryWriteClient
  uid: google.cloud.bigquery_storage_v1.client.BigQueryWriteClient.from_service_account_file
- fullName: google.cloud.bigquery_storage_v1.client.BigQueryWriteClient.from_service_account_info
  isExternal: false
  name: from_service_account_info
  parent: google.cloud.bigquery_storage_v1.client.BigQueryWriteClient
  uid: google.cloud.bigquery_storage_v1.client.BigQueryWriteClient.from_service_account_info
- fullName: google.cloud.bigquery_storage_v1.client.BigQueryWriteClient.from_service_account_json
  isExternal: false
  name: from_service_account_json
  parent: google.cloud.bigquery_storage_v1.client.BigQueryWriteClient
  uid: google.cloud.bigquery_storage_v1.client.BigQueryWriteClient.from_service_account_json
- fullName: google.cloud.bigquery_storage_v1.client.BigQueryWriteClient.get_mtls_endpoint_and_cert_source
  isExternal: false
  name: get_mtls_endpoint_and_cert_source
  parent: google.cloud.bigquery_storage_v1.client.BigQueryWriteClient
  uid: google.cloud.bigquery_storage_v1.client.BigQueryWriteClient.get_mtls_endpoint_and_cert_source
- fullName: google.cloud.bigquery_storage_v1.client.BigQueryWriteClient.get_write_stream
  isExternal: false
  name: get_write_stream
  parent: google.cloud.bigquery_storage_v1.client.BigQueryWriteClient
  uid: google.cloud.bigquery_storage_v1.client.BigQueryWriteClient.get_write_stream
- fullName: google.cloud.bigquery_storage_v1.client.BigQueryWriteClient.parse_common_billing_account_path
  isExternal: false
  name: parse_common_billing_account_path
  parent: google.cloud.bigquery_storage_v1.client.BigQueryWriteClient
  uid: google.cloud.bigquery_storage_v1.client.BigQueryWriteClient.parse_common_billing_account_path
- fullName: google.cloud.bigquery_storage_v1.client.BigQueryWriteClient.parse_common_folder_path
  isExternal: false
  name: parse_common_folder_path
  parent: google.cloud.bigquery_storage_v1.client.BigQueryWriteClient
  uid: google.cloud.bigquery_storage_v1.client.BigQueryWriteClient.parse_common_folder_path
- fullName: google.cloud.bigquery_storage_v1.client.BigQueryWriteClient.parse_common_location_path
  isExternal: false
  name: parse_common_location_path
  parent: google.cloud.bigquery_storage_v1.client.BigQueryWriteClient
  uid: google.cloud.bigquery_storage_v1.client.BigQueryWriteClient.parse_common_location_path
- fullName: google.cloud.bigquery_storage_v1.client.BigQueryWriteClient.parse_common_organization_path
  isExternal: false
  name: parse_common_organization_path
  parent: google.cloud.bigquery_storage_v1.client.BigQueryWriteClient
  uid: google.cloud.bigquery_storage_v1.client.BigQueryWriteClient.parse_common_organization_path
- fullName: google.cloud.bigquery_storage_v1.client.BigQueryWriteClient.parse_common_project_path
  isExternal: false
  name: parse_common_project_path
  parent: google.cloud.bigquery_storage_v1.client.BigQueryWriteClient
  uid: google.cloud.bigquery_storage_v1.client.BigQueryWriteClient.parse_common_project_path
- fullName: google.cloud.bigquery_storage_v1.client.BigQueryWriteClient.parse_table_path
  isExternal: false
  name: parse_table_path
  parent: google.cloud.bigquery_storage_v1.client.BigQueryWriteClient
  uid: google.cloud.bigquery_storage_v1.client.BigQueryWriteClient.parse_table_path
- fullName: google.cloud.bigquery_storage_v1.client.BigQueryWriteClient.parse_write_stream_path
  isExternal: false
  name: parse_write_stream_path
  parent: google.cloud.bigquery_storage_v1.client.BigQueryWriteClient
  uid: google.cloud.bigquery_storage_v1.client.BigQueryWriteClient.parse_write_stream_path
- fullName: google.cloud.bigquery_storage_v1.client.BigQueryWriteClient.table_path
  isExternal: false
  name: table_path
  parent: google.cloud.bigquery_storage_v1.client.BigQueryWriteClient
  uid: google.cloud.bigquery_storage_v1.client.BigQueryWriteClient.table_path
- fullName: google.cloud.bigquery_storage_v1.client.BigQueryWriteClient.transport
  isExternal: false
  name: transport
  parent: google.cloud.bigquery_storage_v1.client.BigQueryWriteClient
  uid: google.cloud.bigquery_storage_v1.client.BigQueryWriteClient.transport
- fullName: google.cloud.bigquery_storage_v1.client.BigQueryWriteClient.write_stream_path
  isExternal: false
  name: write_stream_path
  parent: google.cloud.bigquery_storage_v1.client.BigQueryWriteClient
  uid: google.cloud.bigquery_storage_v1.client.BigQueryWriteClient.write_stream_path
